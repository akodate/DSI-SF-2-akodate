{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Bayesian Statistics\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Return to Frequentist vs. Bayesian\n",
    "\n",
    "As we jump into bayesian statistics, let's return to the difference between Frequentists and Bayesians.\n",
    "\n",
    "Recall from the beginning of the course that:\n",
    "\n",
    "**Frequentists** believe the \"true\" distribution is fixed (and not known). We can infer more more about this \"true\" distribution by engaging in sampling, testing for effects, and studying relevant parameters of the population.\n",
    "\n",
    "**Bayesians** believe that data informs us about the distribution, and as we receive more data our view of the distribution can be updated, further confirming or denying our previous beliefs (but never in certainty).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Interpretations of probability\n",
    "\n",
    "**FREQUENTIST PROBABILITY** \n",
    "\n",
    "Probability is the true number of \"successes\" or \"positive occurrances\" measured across the hypothetical infinite number of samples/events/trials:\n",
    "\n",
    "### $$p = \\lim_{n \\to +\\infty} \\frac{k}{n}$$\n",
    "\n",
    "where:\n",
    "\n",
    "$p$ is the probability of an occurance.\n",
    "\n",
    "$k$ is the number of occurances.\n",
    "\n",
    "$n$ is the number of events.\n",
    "\n",
    "---\n",
    "\n",
    "**BAYESIAN PROBABILITY**\n",
    "\n",
    "Probability is a representation of our uncertainty given what we know and believe to be true. Given a number of observed positive occurances over a number of events *and our prior belief about the true probability of positive occurances,* what is the *distribution of the true probability*?\n",
    "\n",
    "### $$P\\left(\\;true\\;|\\;observed\\;\\right) = \\frac{P\\left(\\;observed\\;|\\;true\\;\\right)}{P(\\;observed\\;)} P\\left(\\;true\\;\\right)$$\n",
    "\n",
    "where:\n",
    "\n",
    "$P\\left(\\;true\\;|\\;observed\\;\\right)$ is the **posterior probability** or **conditional probability**. This is the probability of an occurance given what we observed.\n",
    "\n",
    "$P\\left(\\;observed\\;|\\;true\\;\\right)$ is the **likelihood,** which is the probability of what we observed  given our prior belief about the probability of occurance. \n",
    "\n",
    "${P(\\;observed\\;)}$ is the **total probability** of the observed data. \n",
    "\n",
    "$P\\left(\\;true\\;\\right)$ is the **prior probability** belief. It is what you thought the probability was before observing the events.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes' theorem\n",
    "\n",
    "Some of you might recognize the above formula as Bayes' theorem. Typically Bayes' theorem is written:\n",
    "\n",
    "### $$P\\left(A\\;|\\;B\\right) = \\frac{P\\left(B\\;|\\;A\\right)\\;P\\left(A\\right)}{P(B)}$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$A$ and $B$ are anything that take probabilities (which is essentially everything).\n",
    "\n",
    "$P(B|A)$ and $P(A|B)$ are the probabilities of $B$ conditional on $A$ and vice versa.\n",
    "\n",
    "This is just another way of writing:\n",
    "\n",
    "### $$P\\left(A\\right)\\;P\\left(B\\;|\\;A\\right) = P\\left(B\\right)\\;P\\left(A\\;|\\;B\\right)$$\n",
    "\n",
    "Which is derived from the fact that:\n",
    "\n",
    "### $$P\\left(A\\;\\cap\\;B\\right) = P\\left(A\\right)\\;P\\left(B\\;|\\;A\\right) = P\\left(B\\right)\\;P\\left(A\\;|\\;B\\right)$$\n",
    "\n",
    "Where $P\\left(\\;A\\;\\cap\\;B\\right)$ is the probability of $A$ *and* $B$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Denominator of Bayes' theorem: the \"total probability\"\n",
    "\n",
    "![total probability](./assets/images/output_27_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above picture, we see that each $A_1,..., A_5$ includes a piece of the center oval. In this example the oval represents $B$.\n",
    "\n",
    "Basic probability defines the following relation: $$P(A|B) = \\frac{ A \\cap B }{B}$$ \n",
    "\n",
    "Intuitively, the relation indicates is that $P(A|B)$ is a ratio of the part of A that is common with B, *over the entirety of $B$*. \n",
    "\n",
    "Therefore, **the total probability can be thought of as the exhaustive sum of all probabilities on sets that share elements with B**. This equals simply the probability of B in our set of events.\n",
    "\n",
    "So what is the purpose of the total probability with respect to the rest of Bayes formula? **In essence, it \"normalizes\" the posterior distribution to integrate to 1** ensuring we can interpret this distribution as probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Solving for probability using Bayes' theorem is easy when you know $P(B)$\n",
    "\n",
    "Let's say we have two coins. Coin **FAIR** and coin **RIGGED**\n",
    "\n",
    "    coin FAIR has a 50% chance of flipping heads.\n",
    "    coin RIGGED has 99% chance of flipping heads.\n",
    "    \n",
    "Your friend chooses one of the two coins at random. He flips the coin and gets heads. \n",
    "\n",
    "What is the probability that the coin flipped was **FAIR**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bayes' theorem and modeling\n",
    "\n",
    "We can also interpret Bayes formula in the context of statistical modeling:\n",
    "\n",
    "### $$P\\left(model\\;|\\;data\\right) = \\frac{P\\left(data\\;|\\;model\\right)}{P(data)}\\; P\\left(model\\right)$$\n",
    "\n",
    "What is the probability of our model being true, given the data we have? \n",
    "\n",
    "This depends on the likelihood of the observed data given our model, the probability of the data iself, and our prior belief about the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Back to the coin\n",
    "\n",
    "Say we flip a coin 100 times get heads 58 times. \n",
    "\n",
    "What's the probability that the coin is not fair?\n",
    "\n",
    "We can solve the probability of getting at least this many heads given a probability of 0.5 using the **binomial probability formula:**\n",
    "\n",
    "### $$P(\\;\\text{at least k heads in n flips}\\;) = \\sum_{k}^n\\binom{n}{k}p^kq^{n-k}$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$n$ is the number of flips.\n",
    "\n",
    "$k$ is the number of heads.\n",
    "\n",
    "$p$ is the probability of heads.\n",
    "\n",
    "$q$ is the probability of tails.\n",
    "\n",
    "$\\binom{n}{k}$ describes \"n choose k\" or the possible **combinations of k instances in n events**:\n",
    "\n",
    "### $$\\binom{n}{k} = \\frac{n!}{k!(n-k)!}$$\n",
    "\n",
    "---\n",
    "\n",
    "**Calculate the probability of at least 58 heads in 100 flips with a probability of heads = 0.5.**\n",
    "\n",
    "NOTE: Factorials can be calculated with `np.math.factorial()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set up the info:\n",
    "flips = 100\n",
    "heads = 58\n",
    "prob = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Frequentist hypothesis testing revisited\n",
    "\n",
    "Again, say we flip a coin 100 times get heads 58 times. \n",
    "\n",
    "Say we want to model whether the 58 out of 100 coin is \"rigged\" – not in fact a 50/50 weighted coin – using Frequentist methods.\n",
    "\n",
    "Recall that Frequentists construct a **null hypothesis** and an **alternative hypothesis**.\n",
    "\n",
    "**NULL HYPOTHESIS H0:** The coin is fair. P(heads) = 0.5\n",
    "\n",
    "**ALTERNATIVE HYPOTHESIS HA:** The coin is not fair. P(heads) != 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**THE Z-SCORE:**\n",
    "\n",
    "A **z-score** is a normalized difference between a measured quantity and the quantity's expected value. It is defined as:\n",
    "\n",
    "### $$ z = \\frac{\\bar{x} - E[x]}{std(\\bar{x})}$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$\\bar{x}$ is the empirical (observed) quantity/measurement.\n",
    "\n",
    "$E[x]$ is the expected value of the quantity.\n",
    "\n",
    "$std(\\bar{x})$ is the empirical standard deviation of the quantity.\n",
    "\n",
    "---\n",
    "\n",
    "**THE Z-TEST**\n",
    "\n",
    "The z-test is a standard Frequentist hypothesis test that gives us a p-value (significance) of our measurement. It's validity depends on the Central Limit Theorem.\n",
    "\n",
    "A (two-tailed) p-value is calculated as:\n",
    "\n",
    "### $$\\text{p-value} = P(|z| > z_0) = 2P(z > z_0) = 2(1 - \\Phi(z_0))$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$z_0$ is our measured z-value.\n",
    "\n",
    "$P[|z| > z_0]$ is the probability that the absolute value of z-value is larger than our measured z-value.\n",
    "\n",
    "$2(1-\\Phi(z_0))$ is 2 times 1 minus the cumulative distribution function (CDF) of the normal distribution at point $z_0$.\n",
    "\n",
    "Because the CDF is valued between 0 and 1, essentially **the p-value is the proportion of z-values that are larger than the absolute value of our z-value.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://ipython-books.github.io/images/gaussian.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Coding the z-test\n",
    "\n",
    "We have:\n",
    "\n",
    "    100 coin flips\n",
    "    58 heads\n",
    "    \n",
    "Our hypotheses are:\n",
    "\n",
    "    Null hypothesis H0: probability of heads is 0.5\n",
    "    Alternative hypothesis HA: probability of heads is not 0.5\n",
    "    \n",
    "We want to find the probability that we got 71 heads given that our null hypothesis is true (the p-value).\n",
    "\n",
    "Under the null hypothesis H0:\n",
    "\n",
    "    E[x] = 0.5*flips\n",
    "    Var(x) = (flips * P(heads) * (1 - P(heads))\n",
    "    \n",
    "NOTE: The variance is defined as such because coin flips are part of the [Bernoulli distribution](http://mathworld.wolfram.com/BernoulliDistribution.html).\n",
    "\n",
    "**Calculate the p-value below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the normal distribution.\n",
    "# The function for the CDF is norm.cdf(z)\n",
    "# This takes a z-value as an argument.\n",
    "from scipy.stats import norm\n",
    "\n",
    "# set up our observations\n",
    "flips = 100\n",
    "heads = 58"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bayesian model testing\n",
    "\n",
    "Now let's look at the same scenario but from the Bayesian perspective. \n",
    "\n",
    "### $$P\\left(weight\\;|\\;data\\right) = \\frac{P\\left(data\\;|\\;weight\\right)}{P(data)}\\;P\\left(weight\\right)$$\n",
    "\n",
    "Where $P\\left(\\;bias\\;\\right)$ is the **probability distribution** of getting heads when the coin is flipped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**A UNIFORM PRIOR**\n",
    "\n",
    "You may think we'd assign $P\\left(weight\\right) = 0.5$, but that's not actually correct. \n",
    "\n",
    "$P\\left(weight\\right)$ actually specifies a _distribution of possible biases for our coin_, with some biases potentially having a higher probability than others. \n",
    "\n",
    "When we were using Bayes formula before, we knew the true probability of heads, but now we just have a *distribution of beliefs* about what it might be.\n",
    "\n",
    "If we have no idea, we can set our prior distribution $P\\left(weight\\right)$ to be a **uniform prior**. \n",
    "\n",
    "This indicates that we have no belief about the weighting of the coin whatsoever. We believe all potential weightings are equally likely.\n",
    "\n",
    "With a uniform prior distribution, we set $P\\left(weight\\right) = 1$, which will give all possible biases exactly the same probability. (Imagine a \"distribution\" that is just a flat line.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**THE LIKELIHOOD**\n",
    "\n",
    "Luckily, we have already looked at the likelihood $P\\left(observations\\;|\\;weight\\right)$ earlier. It is the binomial probability function:\n",
    "\n",
    "### $$P\\left(data\\;|\\;weight\\right) = \\binom{flips}{heads}weight^{heads}(1-weight)^{flips-heads}$$\n",
    "\n",
    "This is the probability distribution of the observations given a coin weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**THE TOTAL PROBABILITY OF THE DATA**\n",
    "\n",
    "$P\\left(data\\right)$ is **the probability of our coin flips across all possible coin weights.** We calculate this by integrating over all possible weightings from 0 to 1:\n",
    "\n",
    "### $$P\\left(data\\right) = \\int_{i=0}^1P\\left(data\\;|\\;weight\\right)\\;P\\left(weight\\right)\\;\\partial\\;weight$$\n",
    "\n",
    "The functional use of this is as a \"normalizing factor\" that guarantees the posterior distribution integrates to 1 and is therefore a probability distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**THE POSTERIOR PROBABILITY DISTRIBUTION**\n",
    "\n",
    "Due to nice properties of the binomial distribution and our uniform prior, the posterior distribution can be solved with a formula.\n",
    "\n",
    "For the more detailed math/process behind this, [here is a good ipython notebook](http://ipython-books.github.io/featured-07/)). \n",
    "\n",
    "Keep in mind: tidy solutions like this to posterior distributions are pretty rare. This is an issue we will discuss later when we get to MCMC, which gets around the requirement for a \"closed-form\" solution to the posterior.\n",
    "\n",
    "### $$P\\left(weight\\;|\\;data\\right) = \\frac{\\binom{flips}{heads}weight^{heads}(1-weight)^{flips-heads}}{\\int_{i=0}^1P\\left(data\\;|\\;weight\\right)\\;P\\left(weight\\right)\\;\\partial\\;weight}$$\n",
    "\n",
    "### $$P\\left(weight\\;|\\;data\\right) = (flips + 1)\\binom{flips}{heads}weight^{heads}(1-weight)^{flips-heads}$$\n",
    "\n",
    "**Remember: this is a probability distribution of our coin weighting given the data.**\n",
    "\n",
    "---\n",
    "\n",
    "**Code the posterior and plot it below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import scipy's combination function\n",
    "from scipy.misc import comb\n",
    "\n",
    "# our flips and heads:\n",
    "flips = 100\n",
    "heads = 58\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### More references and sources for lecture:\n",
    "\n",
    "http://ipython-books.github.io/featured-07/\n",
    "\n",
    "http://stats.stackexchange.com/questions/31867/bayesian-vs-frequentist-interpretations-of-probability\n",
    "\n",
    "http://jakevdp.github.io/blog/2014/03/11/frequentism-and-bayesianism-a-practical-intro/\n",
    "\n",
    "https://simple.wikipedia.org/wiki/Bayes%27_theorem\n",
    "\n",
    "https://en.wikipedia.org/wiki/Central_limit_theorem\n",
    "\n",
    "http://www.cogsci.ucsd.edu/classes/SP07/COGS14/NOTES/binomial_ztest.pdf\n",
    "\n",
    "https://en.wikipedia.org/wiki/Prior_probability#Uninformative_priors\n",
    "\n",
    "https://arbital.com/p/bayes_rule/?l=1zq\n",
    "\n",
    "https://betterexplained.com/articles/an-intuitive-and-short-explanation-of-bayes-theorem/\n",
    "\n",
    "http://www.yudkowsky.net/rational/bayes/\n",
    "\n",
    "http://people.stern.nyu.edu/wgreene/MathStat/Notes-2-BayesianStatistics.pdf\n",
    "\n",
    "http://stats.stackexchange.com/questions/58564/help-me-understand-bayesian-prior-and-posterior-distributions\n",
    "\n",
    "http://pages.uoregon.edu/cfulton/posts/bernoulli_trials_bayesian.html\n",
    "\n",
    "http://chrisstrelioff.ws/sandbox/2014/12/11/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations.html\n",
    "\n",
    "https://www.chrisstucchio.com/blog/2013/magic_of_conjugate_priors.html\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
