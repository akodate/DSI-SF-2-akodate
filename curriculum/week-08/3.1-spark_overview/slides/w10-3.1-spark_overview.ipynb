{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```\n",
    "---\n",
    "title: Spark Overview\n",
    "duration: \"1:25\"\n",
    "creator:\n",
    "    name: Francesco Mosconi\n",
    "    city: SF\n",
    "updated:\n",
    "    name: David Yerrington\n",
    "    city: SF\n",
    "---\n",
    "```\n",
    "<img src=\"https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png\" style=\"float: left; margin: 15px;\">\n",
    "\n",
    "# Spark Overview\n",
    "Week 10 | Lesson 3.1\n",
    "\n",
    "![](https://snag.gy/WHA4n0.jpg)\n",
    "![](https://media.licdn.com/mpr/mpr/shrinknp_800_800/AAEAAQAAAAAAAAIOAAAAJDE3ZTdkNjVlLTUxMmEtNDI0MC04NmI0LTBjNmZjY2NiNGFlOA.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LEARNING OBJECTIVES\n",
    "*After this lesson, you will be able to:*\n",
    "- Describe what Spark is / How it works\n",
    "- Identify how Spark is different from Hadoop\n",
    "- Run simple queries in spark from the PySpark console"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### STUDENT PRE-WORK\n",
    "*Before this lesson, you should already be able to:*\n",
    "- perform map reduce jobs with Hadoop\n",
    "- perform map reduce with Python MRJob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LESSON GUIDE\n",
    "| TIMING  | TYPE  | TOPIC  |\n",
    "|:-:|---|---|\n",
    "| 5 min | [Opening](#opening) | Opening |\n",
    "| 20 min | [Introduction](#introduction) | Intro to Spark |\n",
    "| 10 min | [Introduction2](#introduction2) | Spark Stack and API |\n",
    "| 15 min | [Demo](#demo) | Demo: Spark Map Reduce |\n",
    "| 10 min | [Guided-practice](#guided-practice) | Guided Practice: Spark Map Reduce |\n",
    "| 15 min | [Ind-practice](#ind-practice) | Independent Practice: Explore the Spark Shell |\n",
    "| 10 min | [Conclusion](#conclusion) | Conclusion |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<a name=\"opening\"></a>\n",
    "## Opening (5 min)\n",
    "\n",
    "Today will be dedicated to Spark. Spark has brought a revolution in Big Data in the past few years and it is thus important to introduce it and explain how it differes from Hadoop.\n",
    "\n",
    "**Map Reduce Recap** - *Since we did a comprehensive recap yesterday*\n",
    "> It's a framework to solve problems that involve parallel calculation. It's composed of 2 steps: a mapper step in which multiple workers solve the same task on different parts of the dataset and a reducer phase where the results of each workers' work are combined to give a final result.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What limitations have you encountered when processing data with Hadoop?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> You might be inclined to say \"performance\" but map reduce is extremely scalable and can be as performant as Spark in certain cases.  Otherwise, development iterations require more time, increasing time to test and experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"introduction\"></a>\n",
    "## Intro to Spark (20 min)\n",
    "\n",
    "\n",
    "Apache Spark is an open source cluster computing framework. Originally developed at the University of California, Berkeley's [AMPLab](https://amplab.cs.berkeley.edu/), the Spark codebase was later donated to the Apache Software Foundation that has maintained it since. Spark provides an interface for programming entire clusters with implicit data parallelism and fault-tolerance.\n",
    "\n",
    "Spark has gained traction over the past few years because of its superior performace with respect to Hadoop-MapReduce. **In fact, Spark was developed in response to limitations in the MapReduce cluster computing paradigm.** By now you should be familiar with MapReduce. In MapReduce data is read from disk and then a function is mapped across the data. Then the reducer will reduce the results of the map and finally store reduction results back to HDFS.\n",
    "\n",
    "Spark relaxes the constraints of **MR** by doing the following:\n",
    "\n",
    "- Generalizes computation from Map/Reduce only graphs to arbitrary **Directed Acyclic Graphs** (DAGs)\n",
    "- Removes a lot of boilerplate code present in Hadoop\n",
    "- Allows to \"tweak\" parts that in Hadoop are not accessible, like for example the sort algorithm\n",
    "- Allows to load data in a cluster memory, speeding up I/O enormously\n",
    "- Library to leverage many sources of data for input and output\n",
    "\n",
    "![](https://snag.gy/lEfs4v.jpg)\n",
    "\n",
    "The two pillars on which Spark is based are RDDs and DAGs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Resilient Distributed Datasets (RDDs)\n",
    "\n",
    "Apache Spark provides programmers with an application programming interface centered on a data structure called the _resilient distributed dataset (RDD)_, a read-only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way. \n",
    "\n",
    " - Distributed shared memory (vs disk)\n",
    " - Optimal for iterative algorithms (ie: boosted trees)\n",
    " - Tasks can be cached and reused in memory (super fast!)\n",
    " - Easier to write distributed tasks than using map reduce paradigm\n",
    " - 10-100x performance compared to typical map reduce application jobs\n",
    "\n",
    "Spark's RDDs function as a working set for distributed programs that offers a (deliberately) restricted form of distributed shared memory. In other words, Spark is keeping the data in memory, instead of on disk, thus making it easier to implement both iterative algorithms, that visit their dataset multiple times in a loop, and interactive/exploratory data analysis, i.e., the repeated database-style querying of data.\n",
    "\n",
    "The latency of such applications (compared to Apache Hadoop, a popular MapReduce implementation) may be reduced by several orders of magnitude with this approach. Among the class of iterative algorithms are the training algorithms for machine learning systems, which formed the initial impetus for developing Apache Spark. In other words, with Spark we are able to perform Machine Learning at Big Data scale!\n",
    "\n",
    "Apache Spark requires a cluster manager and a distributed storage system. For cluster management, Spark supports standalone (native Spark cluster), Hadoop YARN, or Apache Mesos. For distributed storage, Spark can interface with a wide variety, including Hadoop Distributed File System (HDFS), MapR File System (MapR-FS), Cassandra, OpenStack Swift, Amazon S3, Kudu, or a custom solution can be implemented. Spark also supports a pseudo-distributed local mode, usually used only for development or testing purposes, where distributed storage is not required and the local file system can be used instead; in such a scenario, Spark is run on a single machine with one executor per CPU core. This is how we will use it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Directed Acyclic Graph?\n",
    "DAG (Directed Acyclic Graph) is a programming style for distributed systems - You can think of it as an alternative to Map Reduce. While MR (usually) has just two steps (map and reduce), DAG can have multiple levels that can form a tree structure. Say if you want to execute a SQL query, DAG is more flexible with more functions like map, filter, union etc. Each node in the graph represent a particular operation on the data. \n",
    "\n",
    "- The graph is _Directed_, meaning the information only flows in one direction along the edges and it cannot flow backwards. \n",
    "- This is makes the identification of inputs and outputs easy and unique.\n",
    "\n",
    "![DAG](../assets/images/dag.png)\n",
    "\n",
    "As we will see Spark is smart in how it performs the calculation along the DAG.\n",
    "\n",
    "![Spark](../assets/images/spark-framework.png)\n",
    "\n",
    "**Check:** can you draw a DAG for Hadoop Map Reduce?\n",
    "> Answer: see picture in the assets/images folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"introduction2\"></a>\n",
    "## Spark Stack and API (10 min)\n",
    "\n",
    "![](../assets/images/spark-stack.png)\n",
    "\n",
    "The Spark Core is the foundation of the overall project. It provides distributed task dispatching, scheduling, and basic I/O functionalities, exposed through an application programming interface (for Java, Python, Scala, and R) centered on the RDD abstraction.\n",
    "\n",
    "Here are some of the operations offered by the Spark API:\n",
    "\n",
    "- map\n",
    "- filter\n",
    "- groupby\n",
    "- union\n",
    "- sort\n",
    "- join\n",
    "- reduce\n",
    "- count\n",
    "- fold\n",
    "- reduceByKey\n",
    "- cogroup\n",
    "- cross\n",
    "- zip\n",
    "- sample\n",
    "- take\n",
    "- ....\n",
    "\n",
    "## Does this remind you of anything?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Spark is built in **Scala**, a language derived from Java, that has all the support for functional programing languages, but also has support for OOP languages. Spark builds computation by concatenating functions in the DAG.\n",
    "\n",
    "![](../assets/images/spark-flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scala Tangent\n",
    "\n",
    "Scala is a great language with strong roots in Java.  Features that are developed and released in Spark, are first available through the Scala implementation of Spark libraries.  They are very well tested compared to other languages and usually get the most attention.  Compared to Python, Scala is quite a bit faster, having a much greater flexibility with programming expressions.  Scala is also a very functional and object oriented language by design.\n",
    "\n",
    "```scala\n",
    "  import scala.collection.mutable.Map\n",
    "\n",
    "  object ChecksumAccumulator {\n",
    "\n",
    "    private val cache = Map[String, Int]()\n",
    "\n",
    "    def calculate(s: String): Int = \n",
    "      if (cache.contains(s))\n",
    "        cache(s)\n",
    "      else {\n",
    "        val acc = new ChecksumAccumulator\n",
    "        for (c <- s)\n",
    "          acc.add(c.toByte)\n",
    "        val cs = acc.checksum()\n",
    "        cache += (s -> cs)\n",
    "        cs\n",
    "      }\n",
    "  }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scala vs Python\n",
    "\n",
    "- Learning a Java based language is much more difficult than Python\n",
    "- Deploying Scala requires a bit more effort\n",
    "- Scala is faster than Python in the vast majority of use cases.\n",
    "- There are many more \"types\" in Scala which can be very difficult to grasp for newer programmers.\n",
    "- Python is dynamically typed; Scala is statically typed. \n",
    "- The Python coding process is typically faster than compiled languages (you will code more working Python, faster)\n",
    "\n",
    "** Totally biased opinion from a salty old programmer:**\n",
    "\n",
    "Scala will allow you to learn functional programming (and no, Python is not functional and anyone who says that doesn't understand functional programming.) which will open your mind to a totally new and superior way of reasoning about the programs you write. It will definitely take a lot more work, but the payoff is more than worth it.  Master Python *first*, then *if*, and only *if*, you are interested in higher performance computing with a functional language that is statically typed, Scala is an excellent language.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Spark Variables\n",
    "Spark provides two forms of shared variables:\n",
    "- broadcast variables: they reference read-only data that needs to be available on all nodes\n",
    "- accumulators: they can be used to program reductions in an imperative style\n",
    "\n",
    "> _**Imperative** style programming refers to the idea that you write code that expresses step-by-step operations.  Contrasting to **declaritive** style which you write code that describes what you want, but not necessarily how to get it._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Spark Operations\n",
    "Spark provides two types of operations:\n",
    "- Transformations: these are \"lazy\" operations that only return a result upon \"collect\"\n",
    "- Actions: these are \"non-lazy\" operations that immediately return a result\n",
    "\n",
    "Using lazy operations, we can build a computation graph that only gets executed when we collect the result. This allows Spark to optimize the requested calculation by optimizing the underlying DAG of operations.\n",
    "\n",
    "**Hang tight for the definition of \"Lazy\" in this context.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## (5 Mins) In pairs:  Chooses one of the operations listed above and looks how they work in the documentation. Discuss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"demo\"></a>\n",
    "## Demo: Spark Map Reduce (15 min)\n",
    "(adapted from: http://spark.apache.org/docs/latest/quick-start.html)\n",
    "\n",
    "For the next part we will need our Virtual Machine. By now you should be familiar with how to start it and use it.\n",
    "\n",
    "- Launch Virtualbox\n",
    "- Start Bigdata VM\n",
    "- open a terminal window\n",
    "- connect with vagrant ssh\n",
    "- run bigdata_start.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's open a PySpark shell by typing: `pyspark`. (also jupyter notebook runs a spark context with variable \"sc\")\n",
    "![](../assets/images/pysparkshell.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You should also be able to see an active spark context here: http://10.211.55.101:4040/jobs/\n",
    "\n",
    "![](../assets/images/sparkweb.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's load a text file and perform a few operations:\n",
    "\n",
    "\n",
    "```python\n",
    "textFile = sc.textFile('file:///home/vagrant/data/project_gutenberg/pg11.txt')\n",
    "```\n",
    "\n",
    "We have just created an RDD called `textFile`. As you know, RDDs have actions, which return values, and transformations, which return pointers to new RDDs. Let’s start with a few actions:\n",
    "\n",
    "\n",
    "```python\n",
    ">>> textFile.count() # Number of items in this RDD\n",
    "3735\n",
    "\n",
    ">>> textFile.first() # First item in this RDD\n",
    "u\"Project Gutenberg's Alice's Adventures in Wonderland, by Lewis Carroll\"\n",
    "```\n",
    "\n",
    "Now let’s use a transformation. We will use the filter transformation to return a new RDD with a subset of the items in the file.\n",
    "\n",
    "\n",
    "```python\n",
    ">>> linesWithAlice = textFile.filter(lambda line: \"Alice\" in line)\n",
    "```\n",
    "\n",
    "Notice that the shell returned immediately, since this is a transformation which is lazy. If you type `linesWithAlice`, you should see:\n",
    "\n",
    "\n",
    "```python\n",
    "PythonRDD[10] at RDD at PythonRDD.scala:43\n",
    "```\n",
    "\n",
    "We can chain together transformations and actions:\n",
    "\n",
    "\n",
    "```python\n",
    ">>> textFile.filter(lambda line: \"Alice\" in line).count() # How many lines contain \"Alice\"?\n",
    "396\n",
    "```\n",
    "\n",
    "RDD actions and transformations can be used for more complex computations. Let’s say we want to find the line with the most words:\n",
    "\n",
    "\n",
    "```python\n",
    ">>> textFile.map(lambda line: len(line.split())).reduce(lambda a, b: a if (a > b) else b)\n",
    "18\n",
    "```\n",
    "\n",
    "Let's stop here for a second and review the last line.\n",
    "\n",
    "We started with the `textFile` RDD and we first applied the following map: `lambda line: len(line.split())`\n",
    "\n",
    "**Check:** what does this function do?\n",
    "\n",
    "the we chained to the result of the map the following reduce map: `lambda a, b: a if (a > b) else b`\n",
    "\n",
    "**Check** what does this function do?\n",
    "\n",
    "We used Python anonymous functions (lambdas), but we can also pass any top-level Python function we want. For example, we’ll define a max function to make this code easier to understand:\n",
    "\n",
    "\n",
    "```python\n",
    ">>> def max(a, b):\n",
    "...     if a > b:\n",
    "...         return a\n",
    "...     else:\n",
    "...         return b\n",
    "...\n",
    "\n",
    ">>> textFile.map(lambda line: len(line.split())).reduce(max)\n",
    "18\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"guided-practice\"></a>\n",
    "## Guided Practice: Spark Map Reduce (10 min)\n",
    "### Word count\n",
    "\n",
    "**Check:** how did we implement the word count in Hadoop?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here is how you would implement it in pySpark\n",
    "\n",
    "```python\n",
    ">>> wordCounts = textFile.flatMap(lambda line: line.split()).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a+b)\n",
    "```\n",
    "\n",
    "> What is **FlatMap()** about!? Similar to `map`, but each input item can be mapped to 0 or more output items (so func should return a Seq rather than a single item).  In Spark, `map` returns a single item, whereas `flatMap` can return more than one item.\n",
    "\n",
    "> What does **lazy** mean?  All transformations in Spark are **lazy**, in that they do not compute their results right away. Instead, they just remember the transformations applied to some base dataset (e.g. a file). The transformations are only computed when an action requires a result to be returned to the driver program (ie: **collect()**). This design enables Spark to run more efficiently – for example, we can realize that a dataset created through map will be used in a reduce and return only the result of the reduce to the driver, rather than the larger mapped dataset.\n",
    "\n",
    "> **reduceByKey()**:  When called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function func, which must be of type (V,V) => V. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument.\n",
    "\n",
    "\n",
    "Notice that this is a lazy operation, to collect the word counts in our shell, we can use the collect action:\n",
    "\n",
    "```python\n",
    ">>> wordCounts.collect()\n",
    "[(u\"figure!'\", 1), (u'four', 6), (u'hanging', 3), (u'ringlets', 1), (u\"story!'\", 2), (u'Foundation', 14), ...]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Caching\n",
    "\n",
    "Spark also supports pulling data sets into a cluster-wide in-memory cache. This is very useful when data is accessed repeatedly, such as when querying a small “hot” dataset or when running an iterative algorithm like [PageRank](https://en.wikipedia.org/wiki/PageRank). As a simple example, let’s mark our `linesWithAlice` dataset to be cached:\n",
    "\n",
    "\n",
    "**note:  Try creating a RDD with the Alice in Wonderland wordcounts or just try this with another RDD to see how caching works.**\n",
    "\n",
    "```python\n",
    ">>> linesWithAlice.cache()\n",
    "```\n",
    "\n",
    "Now try running `linesWithAlice.count()` twice. What happens?\n",
    "\n",
    "> Answer: pay attention!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"ind-practice\"></a>\n",
    "## Independent Practice: Explore the Spark Shell  (15 min)\n",
    "\n",
    "### Pi Estimation\n",
    "Let's estimate the value of π by \"throwing darts\" at a circle. We pick random points in the unit square ((0, 0) to (1,1)) and see how many fall in the unit circle. The fraction should be π / 4. Try changing the value of NUM_SAMPLES and see what happens.\n",
    "\n",
    "\n",
    "```python\n",
    "from random import random\n",
    "\n",
    "def sample(p):\n",
    "    x, y = random(), random()\n",
    "    return 1 if x*x + y*y < 1 else 0\n",
    "\n",
    "NUM_SAMPLES = 100000\n",
    "count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b)\n",
    "print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES)\n",
    "```\n",
    "\n",
    "> Pi is roughly 3.137640\n",
    "\n",
    "**Check:** What's going on? Why did we just do that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"conclusion\"></a>\n",
    "## Conclusion: (10 min)\n",
    "\n",
    "We conclude by recap some of the core components of Spark:\n",
    "\n",
    "- **Spark Core:** Contains the basic functionality of Spark; in particular the APIs that define RDDs and the operations and actions that can be undertaken upon them. The rest of Spark's libraries are built on top of the RDD and Spark Core.\n",
    "\n",
    "- **Spark SQL:** Provides APIs for interacting with Spark via the Apache Hive variant of SQL called Hive Query Language (HiveQL). Every database table is represented as an RDD and Spark SQL queries are transformed into Spark operations. For those that are familiar with Hive and HiveQL, Spark can act as a drop-in replacement.\n",
    "\n",
    "- **Spark Streaming:** Enables the processing and manipulation of live streams of data in real time. Many streaming data libraries (such as Apache Storm) exist for handling real-time data. Spark Streaming enables programs to leverage this data similar to how you would interact with a normal RDD as data is flowing in.\n",
    "\n",
    "- **MLlib:** A library of common machine learning algorithms implemented as Spark operations on RDDs. This library contains scalable learning algorithms like classifications, regressions, etc. that require iterative operations across large data sets. The Mahout library, formerly the Big Data machine learning library of choice, will move to Spark for its implementations in the future.\n",
    "\n",
    "- **GraphX:** A collection of algorithms and tools for manipulating graphs and performing parallel graph operations and computations. GraphX extends the RDD API to include operations for manipulating graphs, creating subgraphs, or accessing all vertices in a path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ADDITIONAL RESOURCES\n",
    "\n",
    "- [Spark Slideshare Presentation](http://www.slideshare.net/WillDu1/ten-tools-for-ten-big-data-areas-03apache-spark)\n",
    "- [Quora: what is Apache Spark](https://www.quora.com/What-exactly-is-Apache-Spark-and-how-does-it-work)\n",
    "- [Qubole: Apache Spark Use Cases](https://www.qubole.com/blog/big-data/apache-spark-use-cases)\n",
    "- [Spark Examples](http://spark.apache.org/examples.html)\n",
    "- [Spark getting started](https://districtdatalabs.silvrback.com/getting-started-with-spark-in-python)\n",
    "\n",
    "## BEST GUIDE OVERALL\n",
    "\n",
    "In our biased opinion, the best guide to learning how to develop with Spark, is the official Spark Programming guide.  The examples are very straight forward.  You can see the differences between the languages to get a sense about how to use each.\n",
    "\n",
    "[Spark Programming Guide](http://spark.apache.org/docs/latest/programming-guide.html)\n",
    "\n",
    "Also the book [Advanced Analaytics with Spark](http://shop.oreilly.com/product/0636920035091.do), is a great \"how-to\" style book, although it's examples are all written in Scala -- still a a great book and at the time of this lesson, is the most robust book on this subject on the market given practical application of this technology."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [dsi]",
   "language": "python",
   "name": "Python [dsi]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
