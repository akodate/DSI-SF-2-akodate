{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: HIVE Lab\n",
    "type: lab\n",
    "duration: \"1:25\"\n",
    "creator:\n",
    "    name: Francesco Mosconi\n",
    "    city: SF\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) HIVE Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In the past labs we have introduced Hadoop and MRJob and performed more and more complex map-reduce jobs using those tools.\n",
    "\n",
    "It would be nice however to be able to use the familiar SQL syntax we have learned using relational databases when dealing with Hadoop. Luckily, the the Hadoop ecosystem contains different sub-projects (tools) such as Sqoop, Pig, and Hive that are used to help Hadoop modules and offer that functionality. In particular:\n",
    "\n",
    "- _Sqoop_ is used to import and export data to and from between HDFS and RDBMS.\n",
    "- _Pig_ is a procedural language platform used to develop a script for MapReduce operations.\n",
    "- _Hive_ is a platform used to develop SQL type scripts to do MapReduce operations.\n",
    "\n",
    "In this lab we will focus on **Hive**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hive\n",
    "\n",
    "Hive enables analysis of large data sets using a language very similar to standard ANSI SQL. This means anyone who can write SQL queries can access data stored on the Hadoop cluster. Hive offers a simple interface for:\n",
    "\n",
    "- Log processing\n",
    "- Text mining\n",
    "- Document indexing\n",
    "- Customer-facing business intelligence (e.g., Google Analytics)\n",
    "- Predictive modeling, hypothesis testing\n",
    "\n",
    "Let's start hive by typing `hive` to our VM prompt.\n",
    "\n",
    "**NOTE:** If you turned the VM off, you'll have to re-start all the big data services by running bigdata_start.sh.\n",
    "\n",
    "You should see a prompt like this:\n",
    "\n",
    "    hive>\n",
    "\n",
    "The `SHOW TABLES;` command displays the tables contained:\n",
    "\n",
    "    hive> SHOW TABLES;\n",
    "    \n",
    "**Check:** do you remember the equivalent postgres command?\n",
    "> Answer: \\dt\n",
    "\n",
    "Let's create a table called `gutenberg` where we'll store the word counts for the `project_gutenberg` documents.\n",
    "\n",
    "```sql\n",
    "CREATE EXTERNAL TABLE gutenberg (\n",
    "    word STRING,\n",
    "    count INT\n",
    ")\n",
    "ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t'\n",
    "LOCATION '/user/vagrant/output_gutenberg';\n",
    "```\n",
    "\n",
    "We have just created a table called gutenberg that references the output folder of the `project_gutenberg` hadoop map reduce job we've executed in the past hours.\n",
    "\n",
    "**Check:** go back to the file browser to check what the content of that folder is:\n",
    "\n",
    "    $ hadoop fs -cat /user/vagrant/output_gutenberg/part*\n",
    "\n",
    "Now that we have defined the table in Hive, we can query it using a SQL-like statement:\n",
    "\n",
    "    hive> select * from gutenberg order by count desc limit 10;\n",
    "\n",
    "As you will see, this starts a Map reduce job on the output files and should return something like this:\n",
    "\n",
    "\n",
    "    Total MapReduce CPU Time Spent: 4 seconds 460 msec\n",
    "    OK\n",
    "    the 63656\n",
    "    of  34367\n",
    "    and 32787\n",
    "    to  31399\n",
    "    a   24811\n",
    "    in  18168\n",
    "    I   18070\n",
    "    his 13485\n",
    "    he  13299\n",
    "    was 13029\n",
    "    Time taken: 37.311 seconds, Fetched: 10 row(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Word count in Hive (20 min)\n",
    "\n",
    "Let's go ahead and perform the word count for one of the books in project Gutenberg using Hive.\n",
    "\n",
    "#### 1. Alice in Wonderland word count\n",
    "\n",
    "Let's start by counting the words of Alice in Wonderland (pg11.txt).\n",
    "\n",
    "- create a table called alice_text that will map to the text file lines\n",
    "- create a table called alice that counts the words\n",
    "    - hint: you will need to use the `LATERAL VIEW` keywords to parse the text file table\n",
    "    \n",
    "You can use these 3 resources as reference to find the appropriate commands:\n",
    "\n",
    "- https://www.linkedin.com/pulse/word-count-program-using-r-spark-map-reduce-pig-hive-python-sahu\n",
    "- http://www.hadooplessons.info/2014/12/in-this-post-i-am-going-to-discuss-how.html\n",
    "- http://stackoverflow.com/questions/10039949/word-count-program-in-hive\n",
    "\n",
    "#### 2. Peter Pan word count\n",
    "\n",
    "Repeat the operation creating a new table called peter where you will store the word counts from Peter Pan (pg16.txt).\n",
    "\n",
    "Note that you can get the definition of a table by using the `describe` command:\n",
    "\n",
    "    hive> describe alice;\n",
    "    hive> describe peter;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Joins in Hive (20 min)\n",
    "\n",
    "The advantage of having a SQL-like interface is that it makes join operations much easier to perform.\n",
    "\n",
    "Find the common words to alice and peter table and sort them by the sum of their total count in decreasing order. Limit the display to the first 20 most common words.\n",
    "\n",
    "Result should something like:\n",
    "\n",
    "|word|alice_count|peter_count|sum|\n",
    "|---|---|---|---|\n",
    "|the|1664|2331|3995|\n",
    "|and|780|1396|2176|\n",
    "|...|...|...|...|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3:  HIVE Serialization and DeSerialization (30 min)\n",
    "\n",
    "If your mind is not blown yet by the possibility of doing SQL like queries on hadoop, the next exercise will surely impress you.\n",
    "\n",
    "In fact we can perform SQL queries on documents defining the fields in a regular expression using the keyword SERDE.\n",
    "\n",
    "As an example we have uploaded a log file freely accessible at the address:\n",
    "\n",
    "http://ita.ee.lbl.gov/html/contrib/ClarkNet-HTTP.html\n",
    "\n",
    "and we will parse it in HIVE.\n",
    "\n",
    "As explained on that page, the logs are an ASCII file with one line per request, with the following columns:\n",
    "\n",
    "- host making the request\n",
    "- timestamp, The timezone is -0400.\n",
    "- request given in quotes. (careful sometimes there are additional quotes)\n",
    "- HTTP status reply code\n",
    "- bytes in the reply.\n",
    "\n",
    "\n",
    "In order to parse it we will have to use regular expressions. Since you have not been formally introduced to regular expressions yet, we will walk you through this part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular expressions\n",
    "\n",
    "A regular expression is a sequence of characters that define a search pattern, mainly for use in pattern matching with strings, or string matching, i.e. \"find and replace\"-like operations. They are implemented in python in the module `re`. You can find a cheat sheet of patterns [here](https://www.cheatography.com/davechild/cheat-sheets/regular-expressions/).\n",
    "\n",
    "The `re` module contains several functions to match patterns including\n",
    "- match: to match a pattern at the beginning of a string\n",
    "- search: to match a pattern anywhere in the string\n",
    "- findall: to match all occurrences of the patten in a string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load a few lines of the logs\n",
    "\n",
    "using the gzip module, load a few lines of the [log file](../../assets/datasets/clarknet_access_log_Aug28.gz), and print them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matching ip\n",
    "\n",
    "here's the first 2 lines in the log file:\n",
    "\n",
    "    1: '204.249.225.59 - - [28/Aug/1995:00:00:34 -0400] \"GET /pub/rmharris/catalogs/dawsocat/intro.html HTTP/1.0\" 200 3542\\n'\n",
    "    2: 'access9.accsyst.com - - [28/Aug/1995:00:00:35 -0400] \"GET /pub/robert/past99.gif HTTP/1.0\" 200 4993\\n'\n",
    "\n",
    "Let's write a regular expression to match the ip address that appears at the beginning of the string. Note: various solutions are possible. Note that we have already prepared a couple of test cases for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "line1 = '204.249.225.59 - - [28/Aug/1995:00:00:34 -0400] \"GET /pub/rmharris/catalogs/dawsocat/intro.html HTTP/1.0\" 200 3542\\n'\n",
    "line2 = 'access9.accsyst.com - - [28/Aug/1995:00:00:35 -0400] \"GET /pub/robert/past99.gif HTTP/1.0\" 200 4993\\n'\n",
    "\n",
    "pattern = <WRITE YOUR PATTERN HERE>\n",
    "\n",
    "assert(re.findall(pattern, line1) == ['204.249.225.59'])\n",
    "assert(re.findall(pattern, line2) == ['access9.accsyst.com'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's write a regular expression to match the date and time that appears within squared parenteses. Note: various solutions are possible. You can discard the timezone for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pattern = <WRITE YOUR PATTERN HERE>\n",
    "\n",
    "assert(re.findall(pattern, line1) == ['28/Aug/1995:00:00:34'])\n",
    "assert(re.findall(pattern, line2) == ['28/Aug/1995:00:00:35'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are lost because it's your first time with regular expressions, no worries, below is the solution to the problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_it(line):\n",
    "    return re.findall('^([^ ]+)\\\\s+-\\\\s+-\\\\s+\\\\[([^\\\\]]+)\\s+-0400\\\\]\\\\s+\\\\\"([^ ]+)?\\\\s+([^\\\\\"]+)\\\\s*.*?\\\\\"\\\\s+([^ ]+)\\\\s+([^ ]+)',\n",
    "                      line)\n",
    "\n",
    "assert(get_it('tampico.usc.edu - - [28/Aug/1995:01:04:59 -0400] \"GET / \" 200 1834')==[('tampico.usc.edu', '28/Aug/1995:01:04:59', 'GET', '/ ', '200', '1834')])\n",
    "\n",
    "assert(get_it('cconcepts14.cconcepts.co.uk - - [29/Aug/1995:06:57:43 -0400] \"GET /\" 200 1834')==[('cconcepts14.cconcepts.co.uk',\n",
    "  '29/Aug/1995:06:57:43',\n",
    "  'GET',\n",
    "  '/',\n",
    "  '200',\n",
    "  '1834')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking that I caught all the exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_regex = '^([^ ]+)\\\\s+-\\\\s+-\\\\s+\\\\[([^\\\\]]+)\\s+-0400\\\\]\\\\s+\\\\\"([^ ]+)?\\\\s+([^\\\\\"]+)\\\\s*.*?\\\\\"\\\\s+([^ ]+)\\\\s+([^ ]+)'\n",
    "for l in lines:\n",
    "    ti = re.findall(my_regex, l)\n",
    "    try:\n",
    "        if len(ti[0]) != 6:\n",
    "            print len(ti[0]), ti\n",
    "    except:\n",
    "        print l\n",
    "        print ti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to parse the log in hive.\n",
    "\n",
    "- create a table in HIVE called logs with as many fields as extracted by the regex above.\n",
    "- use the code template below to run the query\n",
    "\n",
    "> Instructor note: They can have some freedom in the how they decide to split each line and which fields they want to extract, but they should at least separate these:\n",
    ">\n",
    "- host STRING,\n",
    "- datetime STRING,\n",
    "- method STRING,\n",
    "- uri STRING,\n",
    "- status INT\n",
    "\n",
    "\n",
    "```sql\n",
    "create external table logs(\n",
    "    <INSERT HERE FIELD DEFINITION>,\n",
    "    <INSERT HERE FIELD DEFINITION>,\n",
    "    )\n",
    "  row format serde 'org.apache.hadoop.hive.serde2.RegexSerDe'\n",
    "  with serdeproperties ( \"input.regex\" = <INSERT HERE YOUR REGEX> );\n",
    "\n",
    "load data local inpath '/home/vagrant/data/logs/clarknet_access_log_Aug28' into table logs;\n",
    "\n",
    "select * from logs limit 20;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Analyze logs\n",
    "\n",
    "Now that we have a nice relational table defined for our logs, let's ask a couple of questions:\n",
    "\n",
    "1. What are the top 10 host that most frequently request pages?\n",
    "- What are the most requested resources (Uri)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Additional Resources\n",
    "\n",
    "- [Serde example](https://community.hortonworks.com/articles/8313/apache-hive-csv-serde-example.html)\n",
    "- [Logs Page](http://ita.ee.lbl.gov/html/contrib/ClarkNet-HTTP.html)\n",
    "- [Cloudera Twitter example](https://github.com/cloudera/cdh-twitter-example)\n",
    "- [AWS Serde example](http://docs.aws.amazon.com/ElasticMapReduce/latest/ManagementGuide/emr-gs.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
