{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "\n",
    "Logistic regression is arguably the most famous and well used classifier. It _is_ a regression, but don't let that confuse you: it estimates probabilities of class membership.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colors = {'blue': '#729ECE',\n",
    "          'brown': '#A8786E',\n",
    "          'green': '#67BF5C',\n",
    "          'grey': '#A2A2A2',\n",
    "          'orange': '#FF9E4A',\n",
    "          'pink': '#ED97CA',\n",
    "          'purple': '#AD8BC9',\n",
    "          'red': '#ED665D',\n",
    "          'teal': '#6DCCDA',\n",
    "          'yellow': '#CDCC5D'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "admissions = pd.read_csv('/Users/kiefer/github-repos/DSI-SF-2/datasets/admissions/admissions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "admissions.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "admit = admissions[admissions.prestige == 1]\n",
    "\n",
    "admit = pd.concat([admit]*10, axis=0)\n",
    "\n",
    "admit.loc[admit.admit == 1, 'gpa'] += np.random.random(size=admit[admit.admit == 1].shape[0])\n",
    "admit.loc[admit.admit == 0, 'gpa'] -= np.random.random(size=admit[admit.admit == 0].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1. A (brief) review of regression models\n",
    "\n",
    "To understand how logistic regression works, we need to understand least squares regression. \n",
    "\n",
    "As you are all familiar with, a regression with variable(s) matrix $X$ predicting target $y$ is formulated as:\n",
    "\n",
    "### $$E(y|X) = \\beta_0 + \\sum_{j}^p\\beta_jx_j$$\n",
    "\n",
    "Where:\n",
    "- $E(y|X)$ is the expected value (mean) of y given variable matrix $X$\n",
    "- $\\sum_{j}^p$ are the predictors $j$ thru $p$ (columns) of the $X$ matrix\n",
    "- $beta_0$ is the intercept\n",
    "- $beta_j$ is the coefficient for the predictor $x_j$, the $j$th column in variable matrix $X$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2. What if we predicted `admit` with `gpa` using regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print 'Intercept:', linmod.intercept_\n",
    "#print 'Coef(s):', linmod.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3. What do those coefficients mean?  Plot admit ~ gpa\n",
    "\n",
    "What is the apparent problem with regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(10,6))\n",
    "# #fig.set_size_inches(6,5)\n",
    "\n",
    "# # plot the regression line for admit ~ gpa\n",
    "# x_vals = np.linspace(1.,5.,300)\n",
    "# ax.plot(x_vals, linmod.predict(x_vals[:, np.newaxis]),\n",
    "#         color='black', alpha=0.7, lw=4)\n",
    "\n",
    "# # do one scatter plot for each type of wine:\n",
    "# ax.scatter(admit.gpa[admit.admit == 0],\n",
    "#            admit.admit[admit.admit == 0],\n",
    "#            c=colors['orange'], s=100, alpha=0.7,\n",
    "#            label='rejected')\n",
    "\n",
    "# ax.scatter(admit.gpa[admit.admit == 1],\n",
    "#            admit.admit[admit.admit == 1],\n",
    "#            c=colors['blue'], s=100, alpha=0.7,\n",
    "#            label='admitted')\n",
    "\n",
    "# ax.set_ylabel('admitted', fontsize=16)\n",
    "# ax.set_xlabel('gpa', fontsize=16)\n",
    "# ax.set_title('admittance ~ gpa, prestige=1\\n', fontsize=20)\n",
    "\n",
    "# ax.set_xlim([1.5,5])\n",
    "# ax.set_ylim(-0.1, 1.1)\n",
    "\n",
    "# plt.legend(loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 5. Binary classes case\n",
    "\n",
    "Logistic regression can solve multi-class problems – we will look at this another day – but the basic classification problem is binary. \n",
    "\n",
    "In our case, `1=admitted` and `0=rejected`.\n",
    "\n",
    "The logistic regression is still solving for an expected value. In the binary classification case this expected value is the probability of one class:\n",
    "\n",
    "### $$E[y \\in {0,1}] = P(y = 1)$$\n",
    "\n",
    "In regression syntax we would have:\n",
    "\n",
    "### $$P(y = 1) = \\beta_0 + \\sum_{j}^p\\beta_jx_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 6. The dilemma: probability estimation using regression\n",
    "\n",
    "### $$P(y = 1) = \\beta_0 + \\sum_{j}^p\\beta_jx_j$$\n",
    "\n",
    "There is an important problem with this new equation: we want to estimate a probability instead of a real number.\n",
    "\n",
    "Why is this a problem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We need $y$ to fall in the range `[-infinity, infinity]` for the regression to be valid!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4. The logit \"link function\"\n",
    "\n",
    "As the name implies, logistic regression is still a regression. It can still be solved by minimization of the sum of squared errors, and there is still an intercept and coefficients.\n",
    "\n",
    "Logistic regression is a twist on regression for categorical/class target variables. Instead of solving for the _mean_ of $y$, logistic regression solves for the _probability of class membership_ of $y$.\n",
    "\n",
    "How does it do this? Regressions can be generalized to $y$ targets that do not fall between `[-infinity, infinity]` through the use of **link functions**.\n",
    "\n",
    "A link function is simply a function of the expected value of the target variable:\n",
    "\n",
    "### $$logit(E(y | X)) = \\beta_0 + \\sum_{j}^p\\beta_jx_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Probabilities are guaranteed to be between 0 and 1.\n",
    "2. The regression formula makes no guarantee that the sum of predictors multiplied by coefficients will sum to a value between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 7. Step 1: Odds ratios\n",
    "\n",
    "We have to modify the regression for it to work for predicting probabilities. The initial step in the solution depends on the use of **odds ratios**. Before we get into _why_, it's important to understand what an odds ratio is.\n",
    "\n",
    "Probabilities and odds ratios represent the same thing in different ways. Probabilities can be alternatively expressed as odds ratios. The odds ratio for probability **p** is defined:\n",
    "\n",
    "### $$\\text{odds ratio}(p) = \\frac{p}{1-p}$$\n",
    "\n",
    "The odds ratio of a probability is a measure of how many times more likely it is than the inverse case.\n",
    "\n",
    "For example:\n",
    "\n",
    "- When **`p = 0.5`**: **`odds ratio = 1`**\n",
    "    - it is equally likely to happen as it is to not happen.\n",
    "- When **`p = 0.75`**: **`odds ratio = 3`**\n",
    "    - it is 3 times more likely to happen than not happen.\n",
    "- When **`p = 0.40`**: **`odds ratio = 0.666..`**\n",
    "    - it is 2/3rds as likely to happen than not happen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the probabilities of admittance by prestige?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print 'P(admit | prestige = 1):', #?\n",
    "# print 'P(admit | prestige = 2):', #?\n",
    "# print 'P(admit | prestige = 3):', #?\n",
    "# print 'P(admit | prestige = 4):', #?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write a function to calculate odds ratios and calculate the odds ratios of admittance by prestige."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print 'odds(admit | prestige = 1):', #?\n",
    "# print 'odds(admit | prestige = 2):', #?\n",
    "# print 'odds(admit | prestige = 3):', #?\n",
    "# print 'odds(admit | prestige = 4):', #?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 8: Odds ratio in place of probability\n",
    "\n",
    "What happens if we put the odds ratio in place of the probability in the regression equation?\n",
    "\n",
    "Put the odds ratio in place of the probability on the left side of the regression equation.\n",
    "\n",
    "### $$ \\frac{P(y = 1)}{1-P(y = 1)} = \\beta_0 + \\sum_{j}^p\\beta_jx_j$$\n",
    "\n",
    "The range of odds ratio, our predicted value, is now restricted to be in the range **`[0, infinity]`**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What happens if we just use the odds ratio as the link function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from scipy.optimize import minimize\n",
    "\n",
    "# min_betas = minimize(minimize_gpa_odds, np.array([0.,1.]))\n",
    "\n",
    "# print min_betas.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(10,6))\n",
    "# #fig.set_size_inches(6,5)\n",
    "\n",
    "# # plot the regression line for admit ~ gpa\n",
    "# x_vals = np.linspace(-20,20,300)\n",
    "# y_vals = min_betas.x[0] + min_betas.x[1]*x_vals\n",
    "# y_vals = pred_to_odds(y_vals)\n",
    "\n",
    "# ax.plot(x_vals, y_vals, color='black', alpha=0.7, lw=4)\n",
    "\n",
    "# # do one scatter plot for each type of wine:\n",
    "# ax.scatter(admit.gpa[admit.admit == 0],\n",
    "#            admit.admit[admit.admit == 0],\n",
    "#            c=colors['orange'], s=100, alpha=0.7,\n",
    "#            label='rejected')\n",
    "\n",
    "# ax.scatter(admit.gpa[admit.admit == 1],\n",
    "#            admit.admit[admit.admit == 1],\n",
    "#            c=colors['blue'], s=100, alpha=0.7,\n",
    "#            label='admitted')\n",
    "\n",
    "# ax.set_ylabel('admitted', fontsize=16)\n",
    "# ax.set_xlabel('gpa', fontsize=16)\n",
    "# ax.set_title('admittance ~ gpa, prestige=1\\n', fontsize=20)\n",
    "\n",
    "# ax.set_xlim([-20,20])\n",
    "# ax.set_ylim([-.3, 3])\n",
    "\n",
    "# plt.legend(loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 9. Step 2: Log odds (natural logarithm of the odds ratio)\n",
    "\n",
    "If we take the natural logarithm of a variable that falls between 0 and infinity, we can actually transform it into a variable that falls between the range negative infinity and infinity.\n",
    "\n",
    "This is because taking the logarithm of fractions results in negative numbers.\n",
    "\n",
    "The regression can now predict any negative or positive number, and we can convert it back into the odds ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# xs = np.linspace(0.001, 5, 500)\n",
    "# ys = np.log(xs)\n",
    "\n",
    "# plt.figure(figsize=[10,6])\n",
    "# plt.axhline(y=0, linewidth=3, c=colors['red'], ls='dashed', alpha=0.3)\n",
    "# plt.plot(xs, ys, lw=4, c='black', alpha=0.7)\n",
    "\n",
    "# plt.xlabel('odds ratio', fontsize=16)\n",
    "# plt.ylabel('log(odds ratio)', fontsize=16)\n",
    "# plt.title('Transformation from odds ratio to log(odds ratio)\\n',fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. The logit link function\n",
    "\n",
    "The combination of converting the probability to an odds ratio and taking the logarithm of that is called the **logit link function**, and is what regression uses to estimate probability:\n",
    "\n",
    "\n",
    "### $$\\text{logit}\\big(E[y]\\big) = \\text{logit}\\big(P(y=1)\\big) = log\\bigg(\\frac{P(y=1)}{1-P(y=1)}\\bigg) =  \\beta_0 + \\sum_{j}^p\\beta_jx_j$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print 'Logreg intercept:', #?\n",
    "# print 'Logreg coef(s):', #?\n",
    "# print 'Logreg predicted probabilities:', #?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "# # plot the logreg regression line for admit ~ gpa\n",
    "# x_vals = np.linspace(-1.,5.,300)\n",
    "# y_pp = logreg.predict_proba(x_vals[:, np.newaxis])[:,1]\n",
    "\n",
    "# ax.plot(x_vals, y_pp, color='black', alpha=0.7, lw=4)\n",
    "\n",
    "# # do one scatter plot for each type of wine:\n",
    "# ax.scatter(admit.gpa[admit.admit == 0],\n",
    "#            admit.admit[admit.admit == 0],\n",
    "#            c=colors['orange'], s=100, alpha=0.7,\n",
    "#            label='rejected')\n",
    "\n",
    "# ax.scatter(admit.gpa[admit.admit == 1],\n",
    "#            admit.admit[admit.admit == 1],\n",
    "#            c=colors['blue'], s=100, alpha=0.7,\n",
    "#            label='admitted')\n",
    "\n",
    "# ax.axvline(xval_chance, lw=3, color=colors['red'], ls='dashed',\n",
    "#            label='gpa where P(y = 1) = 0.5')\n",
    "\n",
    "# ax.set_ylabel('admitted', fontsize=16)\n",
    "# ax.set_xlabel('gpa', fontsize=16)\n",
    "# ax.set_title('admittance ~ gpa, prestige=1\\n', fontsize=20)\n",
    "\n",
    "# ax.set_xlim([2.,5.])\n",
    "# ax.set_ylim([-0.1, 1.1])\n",
    "\n",
    "# plt.legend(loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 11. How do we minimize the RSS? Inverting the logit link function with the \"logistic\"\n",
    "\n",
    "The inverse function of the logit is called the **logistic function**. \n",
    "\n",
    "By inverting the logit, we can have the right side of our regression equation explicitly solving for $P(y = 1)$:\n",
    "\n",
    "### $$P(y=1) = logit^{-1}\\left(\\beta_0 + \\sum_{j}^p\\beta_jx_j\\right)$$\n",
    "\n",
    "Where:\n",
    "\n",
    "### $$logit^{-1}(a) = logistic(a) = \\frac{e^{a}}{e^{a} + 1}$$ \n",
    "\n",
    "Giving us:\n",
    "\n",
    "### $$P(y=1) = \\frac{e^{\\left(\\beta_0 + \\sum_{j}^p\\beta_jx_j\\right)}}{e^{\\left(\\beta_0 + \\sum_{j}^p\\beta_jx_j\\right)}+1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 11. Interpreting logistic regression coefficients.\n",
    "\n",
    "The interpretation of logistic regression coefficients can be counter-intuitive. \n",
    "\n",
    "**To make it easier, start by centering your predictor variables (subtracting the mean):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why does centering matter?\n",
    "\n",
    "Centering helps a lot because now the \"baseline\" for the predictor, the value at 0, is the mean of the predictor. So, in our case, when gpa = 0 this is the average gpa across students.\n",
    "\n",
    "After re-running the regression we can take a look at the coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print 'Logreg intercept:', #?\n",
    "#print 'Logreg coef(s):', #?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Meaning of the betas in log odds\n",
    "\n",
    "Remember that our values are in terms of log-odds. \n",
    "\n",
    "> If $\\beta_1$ is 0, then $\\beta_0$ represents the log odds of admittance for a student with an average gpa.\n",
    "\n",
    "> $\\beta_1$ is the effect of a unit increase in gpa on the log odds of admittance. \n",
    "\n",
    "This sucks because log odds are hard to interpret. Luckily though, we can apply the logistic transform to get the probability of admittance at different $\\beta$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of admittance with an average gpa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print 'average gpa:', #?\n",
    "#print 'P(admitted | average gpa):', "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print 'gpa = 2.5, difference from mean:', #?\n",
    "#print 'P(admitted | gpa = 2.5):', #?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print 'gpa = 4, difference from mean:', #?\n",
    "#print 'P(admitted | gpa = 4):', #?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** With more coefficients, setting $\\beta$ values other than the one of interest to zero (except the intercept) will let you see the change in probability as the corresponding $x$ variable changes.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
