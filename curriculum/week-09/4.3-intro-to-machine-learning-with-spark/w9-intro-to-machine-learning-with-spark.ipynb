{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "---\n",
    "title: Machine Learning with Spark\n",
    "type:  lesson + lab + demo\n",
    "duration: \"1:25\"\n",
    "creator:\n",
    "    name: David Yerrington\n",
    "    city: SF\n",
    "---\n",
    "```\n",
    "<img src=\"https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png\" style=\"float: left; margin: 10px\">\n",
    "\n",
    "#  Intro to:  Machine Learning with Spark\n",
    "Week 9 | 4.3\n",
    "\n",
    "\n",
    "<img src=\"https://snag.gy/vD04Y2.jpg\" width=\"600\">\n",
    "\n",
    "Common cases with data preprocessing and Spark MLib for Machine Learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "Working with RDD's in Spark, can be a bit more challenging with regards to loading data.  The most common method provided for working with text data is `sc.textFile()`.  The data returned will be a semi-structured RDD where each line is cast into rows of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'leaid11,STNAM,FIPST,leanm11,ALL_COHORT_1112,ALL_RATE_1112,MAM_COHORT_1112,MAM_RATE_1112,MAS_COHORT_1112,MAS_RATE_1112,MBL_COHORT_1112,MBL_RATE_1112,MHI_COHORT_1112,MHI_RATE_1112,MTR_COHORT_1112,MTR_RATE_1112,MWH_COHORT_1112,MWH_RATE_1112,CWD_COHORT_1112,CWD_RATE_1112,ECD_COHORT_1112,ECD_RATE_1112,LEP_COHORT_1112,Percentage,State,County,Tract.Code,School.District,District.ID,GIDTR,State.1,State_name,County.1,County_name,Tract,Flag,Num_BGs_in_Tract,LAND_AREA,AIAN_LAND,URBANIZED_AREA_POP_CEN_2010,URBAN_CLUSTER_POP_CEN_2010,RURAL_POP_CEN_2010,Tot_Population_CEN_2010,Tot_Population_ACS_08_12,Tot_Population_ACSMOE_08_12,Males_CEN_2010,Males_ACS_08_12,Males_ACSMOE_08_12,Females_CEN_2010,Females_ACS_08_12,Females_ACSMOE_08_12,Pop_under_5_CEN_2010,Pop_under_5_ACS_08_12,Pop_under_5_ACSMOE_08_12,Pop_5_17_CEN_2010,Pop_5_17_ACS_08_12,Pop_5_17_ACSMOE_08_12,Pop_18_24_CEN_2010,Pop_18_24_ACS_08_12,Pop_18_24_ACSMOE_08_12,Pop_25_44_CEN_2010,Pop_25_44_ACS_08_12,Pop_25_44_ACSMOE_08_12,Pop_45_64_CEN_2010,Pop_45_64_ACS_08_12,Pop_45_64_ACSMOE_08_12,Pop_65plus_CEN_2010,Pop_65plus_ACS_08_12,Pop_65plus_ACSMOE_08_12,Tot_GQ_CEN_2010,Inst_GQ_CEN_2010,Non_Inst_GQ_CEN_2010,Hispanic_CEN_2010,Hispanic_ACS_08_12,Hispanic_ACSMOE_08_12,NH_White_alone_CEN_2010,NH_White_alone_ACS_08_12,NH_White_alone_ACSMOE_08_12,NH_Blk_alone_CEN_2010,NH_Blk_alone_ACS_08_12,NH_Blk_alone_ACSMOE_08_12,NH_AIAN_alone_CEN_2010,NH_AIAN_alone_ACS_08_12,NH_AIAN_alone_ACSMOE_08_12,NH_Asian_alone_CEN_2010,NH_Asian_alone_ACS_08_12,NH_Asian_alone_ACSMOE_08_12,NH_NHOPI_alone_CEN_2010,NH_NHOPI_alone_ACS_08_12,NH_NHOPI_alone_ACSMOE_08_12,NH_SOR_alone_CEN_2010,NH_SOR_alone_ACS_08_12,NH_SOR_alone_ACSMOE_08_12,Pop_5yrs_Over_ACS_08_12,Pop_5yrs_Over_ACSMOE_08_12,Othr_Lang_ACS_08_12,Othr_Lang_ACSMOE_08_12,Age5p_Only_English_ACS_08_12,Age5p_Only_English_ACSMOE_08_12,Age5p_Spanish_ACS_08_12,Age5p_Spanish_ACSMOE_08_12,Age5p_French_ACS_08_12,Age5p_French_ACSMOE_08_12,Age5p_FrCreole_ACS_08_12,Age5p_FrCreole_ACSMOE_08_12,Age5p_Italian_ACS_08_12,Age5p_Italian_ACSMOE_08_12,Age5p_Portuguese_ACS_08_12,Age5p_Portuguese_ACSMOE_08_12,Age5p_German_ACS_08_12,Age5p_German_ACSMOE_08_12,Age5p_Yiddish_ACS_08_12,Age5p_Yiddish_ACSMOE_08_12,Age5p_WGerman_ACS_08_12,Age5p_WGerman_ACSMOE_08_12,Age5p_Scandinavian_ACS_08_12,Age5p_Scandinavian_ACSMOE_08_12,Age5p_Greek_ACS_08_12,Age5p_Greek_ACSMOE_08_12,Age5p_Russian_ACS_08_12,Age5p_Russian_ACSMOE_08_12,Age5p_Polish_ACS_08_12,Age5p_Polish_ACSMOE_08_12,Age5p_SRBCroatian_ACS_08_12,Age5p_SRBCroatian_ACSMOE_08_12,Age5p_OthSlavic_ACS_08_12,Age5p_OthSlavic_ACSMOE_08_12,Age5p_Armenian_ACS_08_12,Age5p_Armenian_ACSMOE_08_12,Age5p_Persian_ACS_08_12,Age5p_Persian_ACSMOE_08_12,Age5p_Gujarati_ACS_08_12,Age5p_Gujarati_ACSMOE_08_12,Age5p_Hindi_ACS_08_12,Age5p_Hindi_ACSMOE_08_12,Age5p_Urdu_ACS_08_12,Age5p_Urdu_ACSMOE_08_12,Age5p_OthIndic_ACS_08_12,Age5p_OthIndic_ACSMOE_08_12,Age5p_OthEuro_ACS_08_12,Age5p_OthEuro_ACSMOE_08_12,Age5p_Chinese_ACS_08_12,Age5p_Chinese_ACSMOE_08_12,Age5p_Japanese_ACS_08_12,Age5p_Japanese_ACSMOE_08_12,Age5p_Korean_ACS_08_12,Age5p_Korean_ACSMOE_08_12,Age5p_Cambodian_ACS_08_12,Age5p_Cambodian_ACSMOE_08_12,Age5p_Hmong_ACS_08_12,Age5p_Hmong_ACSMOE_08_12,Age5p_Thai_ACS_08_12,Age5p_Thai_ACSMOE_08_12,Age5p_Laotian_ACS_08_12,Age5p_Laotian_ACSMOE_08_12,Age5p_Vietnamese_ACS_08_12,Age5p_Vietnamese_ACSMOE_08_12,Age5p_OthAsian_ACS_08_12,Age5p_OthAsian_ACSMOE_08_12,Age5p_Tagalog_ACS_08_12,Age5p_Tagalog_ACSMOE_08_12,Age5p_OthPacIsl_ACS_08_12,Age5p_OthPacIsl_ACSMOE_08_12,Age5p_Navajo_ACS_08_12,Age5p_Navajo_ACSMOE_08_12,Age5p_NativeNAm_ACS_08_12,Age5p_NativeNAm_ACSMOE_08_12,Age5p_Hungarian_ACS_08_12,Age5p_Hungarian_ACSMOE_08_12,Age5p_Arabic_ACS_08_12,Age5p_Arabic_ACSMOE_08_12,Age5p_Hebrew_ACS_08_12,Age5p_Hebrew_ACSMOE_08_12,Age5p_African_ACS_08_12,Age5p_African_ACSMOE_08_12,Age5p_OthUnSp_ACS_08_12,Age5p_OthUnSp_ACSMOE_08_12,Pop_25yrs_Over_ACS_08_12,Pop_25yrs_Over_ACSMOE_08_12,Not_HS_Grad_ACS_08_12,Not_HS_Grad_ACSMOE_08_12,College_ACS_08_12,College_ACSMOE_08_12,Pov_Univ_ACS_08_12,Pov_Univ_ACSMOE_08_12,Prs_Blw_Pov_Lev_ACS_08_12,Prs_Blw_Pov_Lev_ACSMOE_08_12,Civ_labor_16plus_ACS_08_12,Civ_labor_16plus_ACSMOE_08_12,Civ_emp_16plus_ACS_08_12,Civ_emp_16plus_ACSMOE_08_12,Civ_unemp_16plus_ACS_08_12,Civ_unemp_16plus_ACSMOE_08_12,Civ_labor_16_24_ACS_08_12,Civ_labor_16_24_ACSMOE_08_12,Civ_emp_16_24_ACS_08_12,Civ_emp_16_24_ACSMOE_08_12,Civ_unemp_16_24_ACS_08_12,Civ_unemp_16_24_ACSMOE_08_12,Civ_labor_25_44_ACS_08_12,Civ_labor_25_44_ACSMOE_08_12,Civ_emp_25_44_ACS_08_12,Civ_emp_25_44_ACSMOE_08_12,Civ_unemp_25_44_ACS_08_12,Civ_unemp_25_44_ACSMOE_08_12,Civ_labor_45_64_ACS_08_12,Civ_labor_45_64_ACSMOE_08_12,Civ_emp_45_64_ACS_08_12,Civ_emp_45_64_ACSMOE_08_12,Civ_unemp_45_64_ACS_08_12,Civ_unemp_45_64_ACSMOE_08_12,Civ_labor_65plus_ACS_08_12,Civ_labor_65plus_ACSMOE_08_12,Civ_emp_65plus_ACS_08_12,Civ_emp_65plus_ACSMOE_08_12,Civ_unemp_65plus_ACS_08_12,Civ_unemp_65plus_ACSMOE_08_12,Pop_1yr_Over_ACS_08_12,Pop_1yr_Over_ACSMOE_08_12,Diff_HU_1yr_Ago_ACS_08_12,Diff_HU_1yr_Ago_ACSMOE_08_12,Born_US_ACS_08_12,Born_US_ACSMOE_08_12,Born_foreign_ACS_08_12,Born_foreign_ACSMOE_08_12,US_Cit_Nat_ACS_08_12,US_Cit_Nat_ACSMOE_08_12,NON_US_Cit_ACS_08_12,NON_US_Cit_ACSMOE_08_12,ENG_VW_SPAN_ACS_08_12,ENG_VW_SPAN_ACSMOE_08_12,ENG_VW_INDO_EURO_ACS_08_12,ENG_VW_INDO_EURO_ACSMOE_08_12,ENG_VW_API_ACS_08_12,ENG_VW_API_ACSMOE_08_12,ENG_VW_OTHER_ACS_08_12,ENG_VW_OTHER_ACSMOE_08_12,ENG_VW_ACS_08_12,ENG_VW_ACSMOE_08_12,Rel_Family_HHDS_CEN_2010,Rel_Family_HHD_ACS_08_12,Rel_Family_HHD_ACSMOE_08_12,MrdCple_Fmly_HHD_CEN_2010,MrdCple_Fmly_HHD_ACS_08_12,MrdCple_Fmly_HHD_ACSMOE_08_12,Not_MrdCple_HHD_CEN_2010,Not_MrdCple_HHD_ACS_08_12,Not_MrdCple_HHD_ACSMOE_08_12,Female_No_HB_CEN_2010,Female_No_HB_ACS_08_12,Female_No_HB_ACSMOE_08_12,NonFamily_HHD_CEN_2010,NonFamily_HHD_ACS_08_12,NonFamily_HHD_ACSMOE_08_12,Sngl_Prns_HHD_CEN_2010,Sngl_Prns_HHD_ACS_08_12,Sngl_Prns_HHD_ACSMOE_08_12,HHD_PPL_Und_18_CEN_2010,HHD_PPL_Und_18_ACS_08_12,HHD_PPL_Und_18_ACSMOE_08_12,Tot_Prns_in_HHD_CEN_2010,Tot_Prns_in_HHD_ACS_08_12,Tot_Prns_in_HHD_ACSMOE_08_12,Rel_Child_Under_6_CEN_2010,Rel_Child_Under_6_ACS_08_12,Rel_Child_Under_6_ACSMOE_08_12,HHD_Moved_in_ACS_08_12,HHD_Moved_in_ACSMOE_08_12,PUB_ASST_INC_ACS_08_12,PUB_ASST_INC_ACSMOE_08_12,Med_HHD_Inc_ACS_08_12,Med_HHD_Inc_ACSMOE_08_12,Aggregate_HH_INC_ACS_08_12,Aggregate_HH_INC_ACSMOE_08_12,Tot_Housing_Units_CEN_2010,Tot_Housing_Units_ACS_08_12,Tot_Housing_Units_ACSMOE_08_12,Tot_Occp_Units_CEN_2010,Tot_Occp_Units_ACS_08_12,Tot_Occp_Units_ACSMOE_08_12,Tot_Vacant_Units_CEN_2010,Tot_Vacant_Units_ACS_08_12,Tot_Vacant_Units_ACSMOE_08_12,Renter_Occp_HU_CEN_2010,Renter_Occp_HU_ACS_08_12,Renter_Occp_HU_ACSMOE_08_12,Owner_Occp_HU_CEN_2010,Owner_Occp_HU_ACS_08_12,Owner_Occp_HU_ACSMOE_08_12,Single_Unit_ACS_08_12,Single_Unit_ACSMOE_08_12,MLT_U2_9_STRC_ACS_08_12,MLT_U2_9_STRC_ACSMOE_08_12,MLT_U10p_ACS_08_12,MLT_U10p_ACSMOE_08_12,Mobile_Homes_ACS_08_12,Mobile_Homes_ACSMOE_08_12,Crowd_Occp_U_ACS_08_12,Crowd_Occp_U_ACSMOE_08_12,Occp_U_NO_PH_SRVC_ACS_08_12,Occp_U_NO_PH_SRVC_ACSMOE_08_12,No_Plumb_ACS_08_12,No_Plumb_ACSMOE_08_12,Recent_Built_HU_ACS_08_12,Recent_Built_HU_ACSMOE_08_12,Med_House_value_ACS_08_12,Med_House_value_ACSMOE_08_12,Aggr_House_Value_ACS_08_12,Aggr_House_Value_ACSMOE_08_12,MailBack_Area_Count_CEN_2010,TEA_Mail_Out_Mail_Back_CEN_2010,TEA_Update_Leave_CEN_2010,Census_Mail_Returns_CEN_2010,Vacants_CEN_2010,Deletes_CEN_2010,Census_UAA_CEN_2010,Valid_Mailback_Count_CEN_2010,FRST_FRMS_CEN_2010,RPLCMNT_FRMS_CEN_2010,BILQ_Mailout_count_CEN_2010,BILQ_Frms_CEN_2010,Mail_Return_Rate_CEN_2010,Low_Response_Score,pct_URBANIZED_AREA_POP_CEN_2010,pct_URBAN_CLUSTER_POP_CEN_2010,pct_RURAL_POP_CEN_2010,pct_Males_CEN_2010,pct_Males_ACS_08_12,pct_Males_ACSMOE_08_12,pct_Females_CEN_2010,pct_Females_ACS_08_12,pct_Females_ACSMOE_08_12,pct_Pop_Under_5_CEN_2010,pct_Pop_Under_5_ACS_08_12,pct_Pop_Under_5_ACSMOE_08_12,pct_Pop_5_17_CEN_2010,pct_Pop_5_17_ACS_08_12,pct_Pop_5_17_ACSMOE_08_12,pct_Pop_18_24_CEN_2010,pct_Pop_18_24_ACS_08_12,pct_Pop_18_24_ACSMOE_08_12,pct_Pop_25_44_CEN_2010,pct_Pop_25_44_ACS_08_12,pct_Pop_25_44_ACSMOE_08_12,pct_Pop_45_64_CEN_2010,pct_Pop_45_64_ACS_08_12,pct_Pop_45_64_ACSMOE_08_12,pct_Pop_65plus_CEN_2010,pct_Pop_65plus_ACS_08_12,pct_Pop_65plus_ACSMOE_08_12,pct_Tot_GQ_CEN_2010,pct_Inst_GQ_CEN_2010,pct_Non_Inst_GQ_CEN_2010,pct_Hispanic_CEN_2010,pct_Hispanic_ACS_08_12,pct_Hispanic_ACSMOE_08_12,pct_NH_White_alone_CEN_2010,pct_NH_White_alone_ACS_08_12,pct_NH_White_alone_ACSMOE_08_12,pct_NH_Blk_alone_CEN_2010,pct_NH_Blk_alone_ACS_08_12,pct_NH_Blk_alone_ACSMOE_08_12,pct_NH_AIAN_alone_CEN_2010,pct_NH_AIAN_alone_ACS_08_12,pct_NH_AIAN_alone_ACSMOE_08_12,pct_NH_Asian_alone_CEN_2010,pct_NH_Asian_alone_ACS_08_12,pct_NH_Asian_alone_ACSMOE_08_12,pct_NH_NHOPI_alone_CEN_2010,pct_NH_NHOPI_alone_ACS_08_12,pct_NH_NHOPI_alone_ACSMOE_08_12,pct_NH_SOR_alone_CEN_2010,pct_NH_SOR_alone_ACS_08_12,pct_NH_SOR_alone_ACSMOE_08_12,pct_Pop_5yrs_Over_ACS_08_12,pct_Pop_5yrs_Over_ACSMOE_08_12,pct_Othr_Lang_ACS_08_12,pct_Othr_Lang_ACSMOE_08_12,pct_Age5p_Only_Eng_ACS_08_12,pct_Age5p_Only_Eng_ACSMOE_08_12,pct_Age5p_Spanish_ACS_08_12,pct_Age5p_Spanish_ACSMOE_08_12,pct_Age5p_French_ACS_08_12,pct_Age5p_French_ACSMOE_08_12,pct_Age5p_FrCreole_ACS_08_12,pct_Age5p_FrCreole_ACSMOE_08_12,pct_Age5p_Italian_ACS_08_12,pct_Age5p_Italian_ACSMOE_08_12,pct_Age5p_Portugues_ACS_08_12,pct_Age5p_Portugues_ACSMOE_08_12,pct_Age5p_German_ACS_08_12,pct_Age5p_German_ACSMOE_08_12,pct_Age5p_Yiddish_ACS_08_12,pct_Age5p_Yiddish_ACSMOE_08_12,pct_Age5p_WGerman_ACS_08_12,pct_Age5p_WGerman_ACSMOE_08_12,pct_Age5p_Scandinav_ACS_08_12,pct_Age5p_Scandinav_ACSMOE_08_12,pct_Age5p_Greek_ACS_08_12,pct_Age5p_Greek_ACSMOE_08_12,pct_Age5p_Russian_ACS_08_12,pct_Age5p_Russian_ACSMOE_08_12,pct_Age5p_Polish_ACS_08_12,pct_Age5p_Polish_ACSMOE_08_12,pct_Age5p_SRBCroati_ACS_08_12,pct_Age5p_SRBCroati_ACSMOE_08_12,pct_Age5p_OthSlavic_ACS_08_12,pct_Age5p_OthSlavic_ACSMOE_08_12,pct_Age5p_Armenian_ACS_08_12,pct_Age5p_Armenian_ACSMOE_08_12,pct_Age5p_Persian_ACS_08_12,pct_Age5p_Persian_ACSMOE_08_12,pct_Age5p_Gujarati_ACS_08_12,pct_Age5p_Gujarati_ACSMOE_08_12,pct_Age5p_Hindi_ACS_08_12,pct_Age5p_Hindi_ACSMOE_08_12,pct_Age5p_Urdu_ACS_08_12,pct_Age5p_Urdu_ACSMOE_08_12,pct_Age5p_OthIndic_ACS_08_12,pct_Age5p_OthIndic_ACSMOE_08_12,pct_Age5p_OthEuro_ACS_08_12,pct_Age5p_OthEuro_ACSMOE_08_12,pct_Age5p_Chinese_ACS_08_12,pct_Age5p_Chinese_ACSMOE_08_12,pct_Age5p_Japanese_ACS_08_12,pct_Age5p_Japanese_ACSMOE_08_12,pct_Age5p_Korean_ACS_08_12,pct_Age5p_Korean_ACSMOE_08_12,pct_Age5p_Cambodian_ACS_08_12,pct_Age5p_Cambodian_ACSMOE_08_12,pct_Age5p_Hmong_ACS_08_12,pct_Age5p_Hmong_ACSMOE_08_12,pct_Age5p_Thai_ACS_08_12,pct_Age5p_Thai_ACSMOE_08_12,pct_Age5p_Laotian_ACS_08_12,pct_Age5p_Laotian_ACSMOE_08_12,pct_Age5p_Vietnames_ACS_08_12,pct_Age5p_Vietnames_ACSMOE_08_12,pct_Age5p_OthAsian_ACS_08_12,pct_Age5p_OthAsian_ACSMOE_08_12,pct_Age5p_Tagalog_ACS_08_12,pct_Age5p_Tagalog_ACSMOE_08_12,pct_Age5p_OthPacIsl_ACS_08_12,pct_Age5p_OthPacIsl_ACSMOE_08_12,pct_Age5p_Navajo_ACS_08_12,pct_Age5p_Navajo_ACSMOE_08_12,pct_Age5p_NativeNAm_ACS_08_12,pct_Age5p_NativeNAm_ACSMOE_08_12,pct_Age5p_Hungarian_ACS_08_12,pct_Age5p_Hungarian_ACSMOE_08_12,pct_Age5p_Arabic_ACS_08_12,pct_Age5p_Arabic_ACSMOE_08_12,pct_Age5p_Hebrew_ACS_08_12,pct_Age5p_Hebrew_ACSMOE_08_12,pct_Age5p_African_ACS_08_12,pct_Age5p_African_ACSMOE_08_12,pct_Age5p_OthUnSp_ACS_08_12,pct_Age5p_OthUnSp_ACSMOE_08_12,pct_Pop_25yrs_Over_ACS_08_12,pct_Pop_25yrs_Over_ACSMOE_08_12,pct_Not_HS_Grad_ACS_08_12,pct_Not_HS_Grad_ACSMOE_08_12,pct_College_ACS_08_12,pct_College_ACSMOE_08_12,pct_Pov_Univ_ACS_08_12,pct_Pov_Univ_ACSMOE_08_12,pct_Prs_Blw_Pov_Lev_ACS_08_12,pct_Prs_Blw_Pov_Lev_ACSMOE_08_12,pct_Civ_emp_16p_ACS_08_12,pct_Civ_emp_16p_ACSMOE_08_12,pct_Civ_unemp_16p_ACS_08_12,pct_Civ_unemp_16p_ACSMOE_08_12,pct_Civ_emp_16_24_ACS_08_12,pct_Civ_emp_16_24_ACSMOE_08_12,pct_Civ_unemp_16_24_ACS_08_12,pct_Civ_unemp_16_24_ACSMOE_08_12,pct_Civ_emp_25_44_ACS_08_12,pct_Civ_emp_25_44_ACSMOE_08_12,pct_Civ_unemp_25_44_ACS_08_12,pct_Civ_unemp_25_44_ACSMOE_08_12,pct_Civ_emp_45_64_ACS_08_12,pct_Civ_emp_45_64_ACSMOE_08_12,pct_Civ_unemp_45_64_ACS_08_12,pct_Civ_unemp_45_64_ACSMOE_08_12,pct_Civ_emp_65p_ACS_08_12,pct_Civ_emp_65p_ACSMOE_08_12,pct_Civ_unemp_65p_ACS_08_12,pct_Civ_unemp_65p_ACSMOE_08_12,pct_Pop_1yr_Over_ACS_08_12,pct_Pop_1yr_Over_ACSMOE_08_12,pct_Diff_HU_1yr_Ago_ACS_08_12,pct_Diff_HU_1yr_Ago_ACSMOE_08_12,pct_Born_US_ACS_08_12,pct_Born_US_ACSMOE_08_12,pct_Born_foreign_ACS_08_12,pct_Born_foreign_ACSMOE_08_12,pct_US_Cit_Nat_ACS_08_12,pct_US_Cit_Nat_ACSMOE_08_12,pct_NON_US_Cit_ACS_08_12,pct_NON_US_Cit_ACSMOE_08_12,pct_ENG_VW_SPAN_ACS_08_12,pct_ENG_VW_SPAN_ACSMOE_08_12,pct_ENG_VW_INDOEURO_ACS_08_12,pct_ENG_VW_INDOEURO_ACSMOE_08_12,pct_ENG_VW_API_ACS_08_12,pct_ENG_VW_API_ACSMOE_08_12,pct_ENG_VW_OTHER_ACS_08_12,pct_ENG_VW_OTHER_ACSMOE_08_12,pct_ENG_VW_ACS_08_12,pct_ENG_VW_ACSMOE_08_12,pct_Rel_Family_HHDS_CEN_2010,pct_Rel_Family_HHD_ACS_08_12,pct_Rel_Family_HHD_ACSMOE_08_12,pct_MrdCple_HHD_CEN_2010,pct_MrdCple_HHD_ACS_08_12,pct_MrdCple_HHD_ACSMOE_08_12,pct_Not_MrdCple_HHD_CEN_2010,pct_Not_MrdCple_HHD_ACS_08_12,pct_Not_MrdCple_HHD_ACSMOE_08_12,pct_Female_No_HB_CEN_2010,pct_Female_No_HB_ACS_08_12,pct_Female_No_HB_ACSMOE_08_12,pct_NonFamily_HHD_CEN_2010,pct_NonFamily_HHD_ACS_08_12,pct_NonFamily_HHD_ACSMOE_08_12,pct_Sngl_Prns_HHD_CEN_2010,pct_Sngl_Prns_HHD_ACS_08_12,pct_Sngl_Prns_HHD_ACSMOE_08_12,pct_HHD_PPL_Und_18_CEN_2010,pct_HHD_PPL_Und_18_ACS_08_12,pct_HHD_PPL_Und_18_ACSMOE_08_12,avg_Tot_Prns_in_HHD_CEN_2010,avg_Tot_Prns_in_HHD_ACS_08_12,avg_Tot_Prns_in_HHD_ACSMOE_08_12,pct_Rel_Under_6_CEN_2010,pct_Rel_Under_6_ACS_08_12,pct_Rel_Under_6_ACSMOE_08_12,pct_HHD_Moved_in_ACS_08_12,pct_HHD_Moved_in_ACSMOE_08_12,pct_PUB_ASST_INC_ACS_08_12,pct_PUB_ASST_INC_ACSMOE_08_12,pct_Tot_Occp_Units_CEN_2010,pct_Tot_Occp_Units_ACS_08_12,pct_Tot_Occp_Units_ACSMOE_08_12,pct_Vacant_Units_CEN_2010,pct_Vacant_Units_ACS_08_12,pct_Vacant_Units_ACSMOE_08_12,pct_Renter_Occp_HU_CEN_2010,pct_Renter_Occp_HU_ACS_08_12,pct_Renter_Occp_HU_ACSMOE_08_12,pct_Owner_Occp_HU_CEN_2010,pct_Owner_Occp_HU_ACS_08_12,pct_Owner_Occp_HU_ACSMOE_08_12,pct_Single_Unit_ACS_08_12,pct_Single_Unit_ACSMOE_08_12,pct_MLT_U2_9_STRC_ACS_08_12,pct_MLT_U2_9_STRC_ACSMOE_08_12,pct_MLT_U10p_ACS_08_12,pct_MLT_U10p_ACSMOE_08_12,pct_Mobile_Homes_ACS_08_12,pct_Mobile_Homes_ACSMOE_08_12,pct_Crowd_Occp_U_ACS_08_12,pct_Crowd_Occp_U_ACSMOE_08_12,pct_NO_PH_SRVC_ACS_08_12,pct_NO_PH_SRVC_ACSMOE_08_12,pct_No_Plumb_ACS_08_12,pct_No_Plumb_ACSMOE_08_12,pct_Recent_Built_HU_ACS_08_12,pct_Recent_Built_HU_ACSMOE_08_12,pct_TEA_MailOutMailBack_CEN_2010,pct_TEA_Update_Leave_CEN_2010,pct_Census_Mail_Returns_CEN_2010,pct_Vacant_CEN_2010,pct_Deletes_CEN_2010,pct_Census_UAA_CEN_2010,pct_Mailback_Count_CEN_2010,pct_FRST_FRMS_CEN_2010,pct_RPLCMNT_FRMS_CEN_2010,pct_BILQ_Mailout_count_CEN_2010,grad_pct_from_mean',\n",
       " u'3905046,OHIO,39,Wayne Local,128,95.0,,,,,,,,,1.0,PS,127.0,GE95,12.0,GE50,9.0,GE50,,95.87832203,39,165,31000,Wayne Local School District,5046,39165031000,39,Ohio,165,Warren County,31000,,5,44.847,0,3524,0,4656,8180,8267.0,19.0,4079,4246.0,202.0,4101,4021.0,200.0,440,392.0,99.0,1489,1559.0,214.0,584,612.0,153.0,1877,1698.0,229.0,2589,2819.0,212.0,1201,1187.0,169.0,102,94,8,105,141.0,122.0,7906,7969.0,160.0,31,5.0,9.0,17,8.0,12.0,25,66.0,97.0,3,18.0,28.0,0,0.0,16.0,7875.0,100.0,235.0,141.0,7640.0,173.0,58.0,60.0,0.0,16.0,0.0,16.0,0.0,16.0,0.0,16.0,4.0,6.0,0.0,16.0,0.0,16.0,0.0,16.0,0.0,16.0,0.0,16.0,0.0,16.0,0.0,16.0,0.0,16.0,0.0,16.0,0.0,16.0,0.0,16.0,0.0,16.0,0.0,16.0,0.0,16.0,0.0,16.0,0.0,16.0,0.0,16.0,0.0,16.0,0.0,16.0,0.0,16.0,0.0,16.0,0.0,16.0,0.0,16.0,0.0,16.0,0.0,16.0,0.0,16.0,0.0,16.0,0.0,16.0,0.0,16.0,0.0,16.0,0.0,16.0,0.0,16.0,0.0,16.0,5704.0,224.0,663.0,164.0,1477.0,205.0,8176.0,45.0,514.0,251.0,4657.0,293.0,4317.0,314.0,340.0,119.0,665.0,159.0,547.0,141.0,118.0,77.0,1566.0,203.0,1515.0,194.0,51.0,50.0,2166.0,255.0,1995.0,239.0,171.0,94.0,260.0,98.0,260.0,97.0,0.0,16.0,8220.0,38.0,810.0,0.0,8140.0,96.0,127.0,97.0,92.0,84.0,35.0,38.0,0.0,16.0,0.0,16.0,0.0,16.0,0.0,16.0,0.0,16.0,2329,2345.0,153.0,1962,1903.0,154.0,1161,1267.0,223.0,240,246.0,93.0,794,825.0,183.0,671,634.0,165.0,1020,1064.0,110.0,8078,8163.0,56.0,370,305.0,92.0,212.0,88.0,21.0,17.0,\"$64,815\",\"$6,960\",\"$244,172,300\",\"$22,390,481\",3276,3325.0,172.0,3123,3170.0,150.0,153,155.0,79.0,654,516.0,113.0,2469,2654.0,187.0,2810.0,197.0,286.0,95.0,74.0,52.0,155.0,81.0,27.0,27.0,89.0,74.0,15.0,23.0,0.0,16.0,\"$185,500\",\"$11,392\",\"$592,708,500\",\"$85,939,175\",3284.0,3284.0,0.0,2310.0,54.0,0.0,522.0,2708.0,2310.0,0.0,0.0,0.0,85.3,14.53651095,43.08,0.0,56.92,49.87,51.36083223,2.440596901,50.13,48.63916778,2.416673222,5.38,4.741744285,1.197482769,18.2,18.85811056,2.588242436,7.14,7.402927301,1.850653617,22.95,20.53949438,2.769647337,31.65,34.09943148,2.563214912,14.68,14.358292,2.044006044,1.25,1.15,0.1,1.28,1.705576388,1.47574174,96.65,96.39530664,1.922683966,0.38,0.060481432,0.108866489,0.21,0.096770292,0.145155267,0.31,0.798354905,1.17333835,0.04,0.217733156,0.338695651,0.0,0.0,0.193540583,95.25825572,1.189651397,2.984126984,1.790075154,97.01587302,1.818886184,0.736507937,0.761847358,0.0,0.203174603,0.0,0.203174603,0.0,0.203174603,0.0,0.203174603,0.050793651,0.076187746,0.0,0.203174603,0.0,0.203174603,0.0,0.203174603,0.0,0.203174603,0.0,0.203174603,0.0,0.203174603,0.0,0.203174603,0.0,0.203174603,0.0,0.203174603,0.0,0.203174603,0.0,0.203174603,0.0,0.203174603,0.0,0.203174603,0.0,0.203174603,0.0,0.203174603,0.0,0.203174603,0.0,0.203174603,0.0,0.203174603,0.0,0.203174603,0.0,0.203174603,0.0,0.203174603,0.0,0.203174603,0.0,0.203174603,0.0,0.203174603,0.0,0.203174603,0.0,0.203174603,0.0,0.203174603,0.0,0.203174603,0.0,0.203174603,0.0,0.203174603,0.0,0.203174603,0.0,0.203174603,0.0,0.203174603,68.99721785,2.7049239,11.62342216,2.838710547,25.8941094,3.447110435,98.89923793,0.494604086,6.286692759,3.069765859,92.69916255,3.383269707,7.300837449,2.513668599,82.2556391,7.922829028,17.7443609,10.77367422,96.74329502,17.62782879,3.256704981,3.16481513,92.10526316,2.042812279,7.894736842,4.239102019,100.0,53.03370593,0.0,6.153846154,99.43147454,0.398827806,9.854014599,0.045553839,98.46377162,1.138979958,1.536228378,1.173334473,1.112858353,1.016084842,0.423370025,0.459657855,0.0,0.504731861,0.0,0.504731861,0.0,0.504731861,0.0,0.504731861,0.0,0.504731861,74.58,73.97476341,3.323011434,62.82,60.03154574,3.941006372,37.18,39.96845426,6.775704778,7.68,7.760252366,2.910682592,25.42,26.02523659,5.639990908,21.49,20.0,5.118290454,32.66,33.56466877,3.085228218,2.5866,2.575078864,0.123123073,15.89,13.00639659,3.830364088,6.687697161,2.757929278,0.662460568,0.535360673,95.33,95.33834587,6.683873502,4.67,4.661654135,2.363670797,20.94,16.27760252,3.480460272,79.06,83.72239748,4.370851192,84.5112782,3.998942171,8.601503759,2.82228362,2.22556391,1.559666495,4.661654135,2.424125627,0.851735016,0.850780942,2.807570978,2.33060152,0.45112782,0.691335565,0.0,0.481203008,100.0,0.0,70.34,1.64,0.0,15.9,82.46,70.34,0.0,0.0,14.4039505869',\n",
       " u'403290,ARIZONA,4,Ganado Unified School District,152,67.0,149.0,65-69,,,,,2.0,PS,,,,,12.0,GE50,140.0,70-74,2.0,57.31977661,4,1,944901,Ganado Unified District,3290,4001944901,4,Arizona,1,Apache County,944901,,5,576.461,5,0,0,4021,4021,3753.0,466.0,2041,1908.0,275.0,1980,1845.0,232.0,348,263.0,76.0,1055,993.0,214.0,426,368.0,132.0,836,803.0,125.0,924,862.0,136.0,432,464.0,97.0,111,0,111,70,8.0,11.0,80,123.0,49.0,5,40.0,35.0,3778,3542.0,466.0,13,3.0,6.0,3,6.0,8.0,0,0.0,13.0,3490.0,427.0,2417.0,362.0,1073.0,227.0,0.0,13.0,15.0,34.0,0.0,13.0,0.0,13.0,0.0,13.0,0.0,13.0,0.0,13.0,0.0,13.0,0.0,13.0,0.0,13.0,0.0,13.0,0.0,13.0,0.0,13.0,0.0,13.0,0.0,13.0,0.0,13.0,0.0,13.0,0.0,13.0,0.0,13.0,0.0,13.0,0.0,13.0,0.0,13.0,0.0,13.0,0.0,13.0,0.0,13.0,0.0,13.0,0.0,13.0,0.0,13.0,0.0,13.0,0.0,13.0,0.0,13.0,0.0,13.0,645.0,108.0,26.0,38.0,0.0,13.0,0.0,13.0,0.0,13.0,0.0,13.0,0.0,13.0,2129.0,244.0,697.0,118.0,187.0,48.0,3723.0,468.0,1403.0,315.0,1101.0,159.0,814.0,129.0,287.0,71.0,196.0,59.0,72.0,39.0,124.0,44.0,495.0,90.0,394.0,79.0,101.0,51.0,365.0,82.0,306.0,76.0,59.0,32.0,45.0,41.0,42.0,38.0,3.0,14.0,3711.0,461.0,369.0,0.0,3731.0,455.0,22.0,36.0,7.0,8.0,15.0,34.0,0.0,13.0,0.0,13.0,0.0,13.0,218.0,47.0,218.0,49.0,848,592.0,71.0,445,281.0,60.0,762,651.0,73.0,286,236.0,41.0,359,340.0,52.0,321,327.0,53.0,533,327.0,56.0,3910,3664.0,468.0,264,131.0,39.0,73.0,24.0,131.0,39.0,\"$23,065\",\"$3,474\",\"$29,882,300\",\"$3,892,256\",1810,1940.0,65.0,1207,932.0,71.0,603,1008.0,70.0,287,248.0,47.0,920,684.0,62.0,1526.0,93.0,96.0,29.0,0.0,13.0,318.0,49.0,185.0,84.0,172.0,40.0,1148.0,85.0,3.0,5.0,\"$57,900\",\"$13,296\",\"$122,091,900\",\"$47,776,201\",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,,30.27843577,0.0,0.0,100.0,50.76,50.83932854,3.720634612,49.24,49.16067146,0.976236418,8.65,7.007727152,1.828574926,26.24,26.45883293,4.660542447,10.59,9.805488942,3.299733435,20.79,21.39621636,2.008790851,22.98,22.96829203,2.235686125,10.74,12.36344258,2.079305255,2.76,0.0,2.76,1.74,0.213162803,0.291901338,1.99,3.277378098,1.240583089,0.12,1.065814016,0.923149673,93.96,94.37783107,4.104711741,0.32,0.079936051,0.159563701,0.07,0.159872102,0.212236478,0.0,0.0,0.346389555,92.99227285,16.21027469,69.25501433,5.982594325,30.74498567,5.306221691,0.0,0.372492837,0.429799427,0.972791768,0.0,0.372492837,0.0,0.372492837,0.0,0.372492837,0.0,0.372492837,0.0,0.372492837,0.0,0.372492837,0.0,0.372492837,0.0,0.372492837,0.0,0.372492837,0.0,0.372492837,0.0,0.372492837,0.0,0.372492837,0.0,0.372492837,0.0,0.372492837,0.0,0.372492837,0.0,0.372492837,0.0,0.372492837,0.0,0.372492837,0.0,0.372492837,0.0,0.372492837,0.0,0.372492837,0.0,0.372492837,0.0,0.372492837,0.0,0.372492837,0.0,0.372492837,0.0,0.372492837,0.0,0.372492837,0.0,0.372492837,0.0,0.372492837,0.0,0.372492837,18.48137536,2.112653163,0.744985673,1.085003349,0.0,0.372492837,0.0,0.372492837,0.0,0.372492837,0.0,0.372492837,0.0,0.372492837,56.72795097,9.585592601,32.73837482,4.07938052,8.783466416,2.017368953,99.20063949,1.944534306,37.68466291,7.010457982,73.93278837,4.825154823,26.06721163,5.235861232,36.73469388,16.54242401,63.26530612,11.88600443,79.5959596,6.728308244,20.4040404,9.61195235,83.83561644,8.878135873,16.16438356,7.979662751,93.33333333,93.33333333,6.666666667,30.512405,98.88089528,0.375131672,9.943411479,1.235223038,99.41380229,17.30189386,0.586197709,0.956467102,0.186517453,0.211900977,0.399680256,0.904581609,0.0,1.394849785,0.0,1.394849785,0.0,1.394849785,23.39055794,4.717612052,23.39055794,4.946337635,70.26,63.51931331,5.883807643,36.87,30.15021459,6.014094686,63.13,69.84978541,5.747608685,23.7,25.32188841,3.953643654,29.74,36.4806867,4.838000921,26.59,35.08583691,5.019400988,44.16,35.08583691,5.381353088,3.2394,3.931330472,0.403058813,31.13,22.12837838,6.029624183,7.832618026,2.505022476,14.05579399,4.045231309,66.69,48.04123711,3.286819891,33.31,51.95876289,3.160500602,23.78,26.60944206,4.61755723,76.22,73.39055794,3.604942146,78.65979381,4.004341578,4.948453608,1.485622241,0.0,0.670103093,16.39175258,2.465339852,19.84978541,8.885116332,18.45493562,4.055043528,59.17525773,3.907178089,0.154639175,0.257679874,,,,,,,,,,,-19.3151085335']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sc.textFile(\"../../../datasets/data_for_diplomas/grad_test.csv\")\n",
    "data.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD with Schema\n",
    "\n",
    "By default, loading data from textFile has no real _structure_, beyond each line is a string.  It needs to be typed into a schema, which is something that is ususally automatically handled with Pandas when we use **.load_csv()**.  \n",
    "\n",
    "### Partial schema loading with Databricks com.databricks.spark.csv\n",
    "\n",
    "While not perfect, there are a number of packages available within the Spark/Java/Scala ecosystem that are compatible with Pyspark.  One such package is the **spark-csv** package from Databricks.  In Java and Scala, there are package managers like _Maven_, however, in our manually configured Pyspark world, we don't really have a way to do automatic package resolution so we have to manually download these packages in order to get them to show up contextually in our notebook environments.\n",
    "\n",
    "### Installing com.databricks.spark.csv\n",
    "Downloading this file to your **/usr/local/Cellar/apache-spark/2.0.0/libexec/python/lib** directory, should allow you to use the spark-csv package:\n",
    "```bash\n",
    "# On OSX, if you installed your Spark package from brew\n",
    "cd /usr/local/Cellar/apache-spark/2.0.0/libexec/python/lib\n",
    "wget https://repo1.maven.org/maven2/com/databricks/spark-csv_2.11/1.5.0/spark-csv_2.11-1.5.0-sources.jar\n",
    "```\n",
    "\n",
    "The only requirement is that this package / jar file is in your $SPARK_HOME path.\n",
    "\n",
    "> _The spark-csv package isn't without it's problems.  It can be sensitive to column names with special characters._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Store Number: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Zip Code: string (nullable = true)\n",
      " |-- County Number: string (nullable = true)\n",
      " |-- County: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Category Name: string (nullable = true)\n",
      " |-- Vendor Number: string (nullable = true)\n",
      " |-- Item Number: string (nullable = true)\n",
      " |-- Item Description: string (nullable = true)\n",
      " |-- Bottle Volume (ml): string (nullable = true)\n",
      " |-- State Bottle Cost: string (nullable = true)\n",
      " |-- State Bottle Retail: string (nullable = true)\n",
      " |-- Bottles Sold: string (nullable = true)\n",
      " |-- Sale (Dollars): string (nullable = true)\n",
      " |-- Volume Sold (Liters): string (nullable = true)\n",
      " |-- Volume Sold (Gallons): string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").load(\"../../../datasets/iowa_liquor/Iowa_Liquor_sales_sample_10pct.csv\")\n",
    "rdd.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read.csv in Spark 2.0+\n",
    "There's a useful CSV reading method in the latest version of Spark.  You can pass a schema object to it in order to apply a more suitable type at the time of initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|summary|\n",
      "+-------+\n",
      "|  count|\n",
      "|   mean|\n",
      "| stddev|\n",
      "|    min|\n",
      "|    max|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\n",
    "    \"../../../datasets/iowa_liquor/Iowa_Liquor_sales_sample_10pct.csv\", header=True, mode=\"DROPMALFORMED\"\n",
    ")\n",
    "\n",
    "# rdd.printSchema()\n",
    "df.select(\"Store Number\").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Types / Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------------+------------+--------------+\n",
      "|      Date|Store Number|       Category Name|Bottles Sold|Sale (Dollars)|\n",
      "+----------+------------+--------------------+------------+--------------+\n",
      "|11/04/2015|        3717|    APRICOT BRANDIES|          12|        $81.00|\n",
      "|03/02/2016|        2614|    BLENDED WHISKIES|           2|        $41.26|\n",
      "|02/11/2016|        2106|STRAIGHT BOURBON ...|          24|       $453.36|\n",
      "|02/03/2016|        2501|  AMERICAN COCKTAILS|           6|        $85.50|\n",
      "|08/18/2015|        3654|      VODKA 80 PROOF|          12|       $129.60|\n",
      "+----------+------------+--------------------+------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"Date\", \"Store Number\", \"Category Name\", \"Bottles Sold\", \"Sale (Dollars)\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# crap = df.select('Sale (Dollars)').na\n",
    "# crap.replace(\"$\", \"\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Store Number: integer (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Zip Code: integer (nullable = true)\n",
      " |-- County Number: integer (nullable = true)\n",
      " |-- County: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Category Name: string (nullable = true)\n",
      " |-- Vendor Number: integer (nullable = true)\n",
      " |-- Item Number: integer (nullable = true)\n",
      " |-- Item Description: string (nullable = true)\n",
      " |-- Bottle Volume (ml): integer (nullable = true)\n",
      " |-- State Bottle Cost: string (nullable = true)\n",
      " |-- State Bottle Retail: string (nullable = true)\n",
      " |-- Bottles Sold: integer (nullable = true)\n",
      " |-- Sale (Dollars): double (nullable = true)\n",
      " |-- Volume Sold (Liters): double (nullable = true)\n",
      " |-- Volume Sold (Gallons): double (nullable = true)\n",
      "\n",
      "+----------+------------+-----------+--------+-------------+----------+---------+--------------------+-------------+-----------+--------------------+------------------+-----------------+-------------------+------------+--------------+--------------------+---------------------+\n",
      "|      Date|Store Number|       City|Zip Code|County Number|    County| Category|       Category Name|Vendor Number|Item Number|    Item Description|Bottle Volume (ml)|State Bottle Cost|State Bottle Retail|Bottles Sold|Sale (Dollars)|Volume Sold (Liters)|Volume Sold (Gallons)|\n",
      "+----------+------------+-----------+--------+-------------+----------+---------+--------------------+-------------+-----------+--------------------+------------------+-----------------+-------------------+------------+--------------+--------------------+---------------------+\n",
      "|11/04/2015|        3717|     SUMNER|   50674|            9|    Bremer|1051100.0|    APRICOT BRANDIES|           55|      54436|Mr. Boston Aprico...|               750|             4.50|               6.75|          12|          81.0|                 9.0|                 2.38|\n",
      "|03/02/2016|        2614|  DAVENPORT|   52807|           82|     Scott|1011100.0|    BLENDED WHISKIES|          395|      27605|             Tin Cup|               750|            13.75|              20.63|           2|         41.26|                 1.5|                  0.4|\n",
      "|02/11/2016|        2106|CEDAR FALLS|   50613|            7|Black Hawk|1011200.0|STRAIGHT BOURBON ...|           65|      19067|            Jim Beam|              1000|            12.59|              18.89|          24|        453.36|                24.0|                 6.34|\n",
      "|02/03/2016|        2501|       AMES|   50010|           85|     Story|1071100.0|  AMERICAN COCKTAILS|          395|      59154|1800 Ultimate Mar...|              1750|             9.50|              14.25|           6|          85.5|                10.5|                 2.77|\n",
      "|08/18/2015|        3654|    BELMOND|   50421|           99|    Wright|1031080.0|      VODKA 80 PROOF|          297|      35918|  Five O'clock Vodka|              1750|             7.20|              10.80|          12|         129.6|                21.0|                 5.55|\n",
      "+----------+------------+-----------+--------+-------------+----------+---------+--------------------+-------------+-----------+--------------------+------------------+-----------------+-------------------+------------+--------------+--------------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType, IntegerType, DoubleType\n",
    "from pyspark.sql.functions import udf, regexp_replace\n",
    "\n",
    "# stripDollarSigns = udf(lambda s: s.replace(\"$\", \"\"), DoubleType())\n",
    "\n",
    "df = df \\\n",
    ".withColumn(\"Store Number\",          df[\"Store Number\"].cast(\"integer\")) \\\n",
    ".withColumn(\"Sale (Dollars)\",        regexp_replace(\"Sale (Dollars)\", \"\\\\$\", \"\").cast(\"double\")) \\\n",
    ".withColumn(\"Zip Code\",              df[\"Zip Code\"].cast(\"integer\")) \\\n",
    ".withColumn(\"County Number\",         df[\"County Number\"].cast(\"integer\")) \\\n",
    ".withColumn(\"Vendor Number\",         df[\"Vendor Number\"].cast(\"integer\")) \\\n",
    ".withColumn(\"Item Number\",           df[\"Item Number\"].cast(\"integer\")) \\\n",
    ".withColumn(\"Bottle Volume (ml)\",    df[\"Bottle Volume (ml)\"].cast(\"integer\")) \\\n",
    ".withColumn(\"State Bottle Cost\",     regexp_replace(\"State Bottle Cost\", \"\\\\$\", \"\")) \\\n",
    ".withColumn(\"State Bottle Retail\",   regexp_replace(\"State Bottle Retail\", \"\\\\$\", \"\")) \\\n",
    ".withColumn(\"Bottles Sold\",          df[\"Bottles Sold\"].cast(\"integer\")) \\\n",
    ".withColumn(\"Volume Sold (Liters)\",  df[\"Volume Sold (Liters)\"].cast(\"double\")) \\\n",
    ".withColumn(\"Volume Sold (Gallons)\", df[\"Volume Sold (Gallons)\"].cast(\"double\")) \\\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Summary Statistics\n",
    "Once type is defined, describe / show will report useful statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+-----------------+------------------+--------------------+\n",
      "|summary|         Zip Code|Bottle Volume (ml)|     Bottles Sold|    Sale (Dollars)|Volume Sold (Liters)|\n",
      "+-------+-----------------+------------------+-----------------+------------------+--------------------+\n",
      "|  count|           270738|            270955|           270955|            270955|              270955|\n",
      "|   mean|51264.20559729332| 924.8303408315033|9.871284899706593| 128.9023747485706|   8.981351183775748|\n",
      "| stddev|988.9071803701167|493.08848860663403|24.04091157393874|383.02736884240466|  28.913690130072464|\n",
      "|    min|            50002|                50|                1|              1.34|                 0.1|\n",
      "|    max|            56201|              6000|             2508|           36392.4|              2508.0|\n",
      "+-------+-----------------+------------------+-----------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([\"Zip Code\", \"Bottle Volume (ml)\", \"Bottles Sold\", \"Sale (Dollars)\", \"Volume Sold (Liters)\"]).describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The **Spark** equivelent to **Pandas** df.describe() is: \n",
    "> ```python \n",
    "> df.select(df.columns).describe().show()\n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Notes About Matrix/Vector Types in Spark\n",
    "\n",
    "There are lots of types to familliarize yourself with inside of Spark/MLib.  Generally, It is important to choose the right format with storing large and distributed matrices.\n",
    "\n",
    "_\"MLlib supports **local vectors** and matrices stored on a single machine, as well as **distributed matrices** backed by one or more RDDs. **Local vectors** and **local matrices** are simple data models that serve as public interfaces. The underlying linear algebra operations are provided by [Breeze](http://www.scalanlp.org/). A training example used in supervised learning is called a **“labeled point”** in MLlib.\"_\n",
    "\n",
    "> ### Local Vectors\n",
    "\n",
    "> A local vector has integer-typed and 0-based indices and double-typed values, stored on a single machine. MLlib supports two types of local vectors: dense and sparse. A dense vector is backed by a double array representing its entry values, while a sparse vector is backed by two parallel arrays: indices and values. For example, a vector (1.0, 0.0, 3.0) can be represented in dense format as [1.0, 0.0, 3.0] or in sparse format as (3, [0, 2], [1.0, 3.0]), where 3 is the size of the vector.\n",
    ">\n",
    "\n",
    "> ### Labeled Points\n",
    "> A labeled point is a local vector, either dense or sparse, associated with a label/response. In MLlib, labeled points are used in supervised learning algorithms. We use a double to store a label, so we can use labeled points in both regression and classification. For binary classification, a label should be either 0 (negative) or 1 (positive). For multiclass classification, labels should be class indices starting from zero: 0, 1, 2, ....\n",
    "\n",
    "Further reading about Spark matrix / vector types:\n",
    "http://spark.apache.org/docs/latest/mllib-data-types.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing our training data\n",
    "\n",
    "First, we need to take our dataframe, and encode the training features / predictors.  Notice our response variable is the first parameter.\n",
    "\n",
    "> **LabeledPoint(response, [features])**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint, LinearRegressionWithSGD, LinearRegressionModel\n",
    "\n",
    "features = [\"Bottles Sold\", \"Sale (Dollars)\", \"Bottle Volume (ml)\"]\n",
    "response = \"Volume Sold (Liters)\"\n",
    "\n",
    "X = df.rdd.map( \n",
    "    lambda row: LabeledPoint(row[response], [row[feature] for feature in features])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test / Train Split\n",
    "\n",
    "It's also possible to do KFolds, but here is the equivelent to Scikit-learn's `train_test_split()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (30% held out for testing)\n",
    "trainingData, testData = X.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression in Spark\n",
    "\n",
    ">Train a linear regression model using Stochastic Gradient\n",
    "Descent (SGD). This solves the least squares regression\n",
    "formulation\n",
    "\n",
    ">    `f(weights) = 1/(2n) ||A weights - y||^2`\n",
    "\n",
    "Read about the SGD parameters here:<br>\n",
    "http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.regression.LinearRegressionWithSGD\n",
    "\n",
    "The default step paramter is 1.0.  An optimal value for the step hyperparameter is dependant on the data size, and will vary depending on training, testing, and out of sample data.  Plotting the error rate to see how SGD converges is the best way to tune this paramter.  For the sake of this example, step/rate was chosen very roughly to give a cursory sense of the application of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "linearModel = LinearRegressionWithSGD.train(trainingData, iterations=100, step=0.000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Bottles Sold', 0.0029077298508317084),\n",
       " ('Sale (Dollars)', 0.038886120244748495),\n",
       " ('Bottle Volume (ml)', 0.005249214168329245)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip(features, linearModel.weights.array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Familliar Regression Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                R2:  0.619513\n",
      "Explained Variance:  248.598001\n",
      "               MSE:  324.671191\n",
      "              RMSE:  18.018635\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "\n",
    "prediObserRDD = testDataData.map(lambda row: (float(linearModel.predict(row.features)), row.label)).cache()\n",
    "metrics = RegressionMetrics(prediObserRDD)\n",
    "\n",
    "print \"\"\"\n",
    "                R2:  %.6f\n",
    "Explained Variance:  %.6f\n",
    "               MSE:  %.6f\n",
    "              RMSE:  %.6f\n",
    "\"\"\" % (metrics.r2, metrics.explainedVariance, metrics.meanSquaredError, metrics.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## No Problem?\n",
    "\n",
    "By now you might see there are some similarities between Pandas DataFrames and Spark DataFrames.  The nuance and complexity lies with Sparks many types of matrices and datatypes that are specific to your application.  Your application largely depends on the scale of your problem, and the model(s) you choose to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Similarly implemented, the logistic regression model we've become familliar with is implemented in Spark as well as many other models that are available in Scikit-learn.  We will attempt to demonstrate a more concise example using a prior dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load / Clean / Apply Schema\n",
    "This time, we will attept to apply a schema at loadtime of our model.\n",
    "\n",
    "> Note:  If you don't see your data show up after applying a schema, it's likely that you forgot a field or that your supplied schema doesn't match your dataset 1:1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+-------+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|   Fare|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+-------+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|   7.25|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|71.2833|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|  7.925|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|   53.1|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|   8.05|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"PassengerId\", IntegerType()),\n",
    "    StructField(\"Survived\",    IntegerType()),\n",
    "    StructField(\"Pclass\",      IntegerType()),\n",
    "    StructField(\"Name\",        StringType()),\n",
    "    StructField(\"Sex\",         StringType()),\n",
    "    StructField(\"Age\",         DoubleType()),\n",
    "    StructField(\"SibSp\",       IntegerType()),\n",
    "    StructField(\"Parch\",       IntegerType()),\n",
    "    StructField(\"Fare\",        DoubleType()),\n",
    "    StructField(\"Embarked\",    StringType()) \n",
    "])\n",
    "\n",
    "df = spark.read.csv(\n",
    "    \"../../../datasets/titanic/titanic_clean.csv\", header=True, mode=\"DROPMALFORMED\", schema=schema\n",
    ")\n",
    "\n",
    "# Print schema, and then show the first 5 records in printed format\n",
    "df.printSchema()\n",
    "df.show(5)\n",
    "\n",
    "# rdd.printSchema()\n",
    "# df.select(df.columns).describe().show()\n",
    "# df.printSchema()\n",
    "\n",
    "\n",
    "# Build the model\n",
    "# model = LogisticRegressionWithLBFGS.train(parsedData)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Labeled Points\n",
    "\n",
    "Time to create our training data and train test splits.\n",
    "\n",
    "> **LabeledPoint(response, [features])**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = [\"Pclass\", \"Age\", \"SibSp\", \"Parch\"]\n",
    "response = \"Survived\"\n",
    "\n",
    "X = df.rdd.map( \n",
    "    lambda row: LabeledPoint(row[response], [row[feature] for feature in features])\n",
    ")\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "trainingData, testData = X.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Logistic Model\n",
    "\n",
    "Just like Scikit-learn, Spark's Mlib follows a consistent pattern.  Learn the application of one model, the rest quickly become familliar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "\n",
    "logisticModel = LogisticRegressionWithLBFGS.train(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pclass', -0.48155698411683606),\n",
       " ('Age', 0.013819023434247152),\n",
       " ('SibSp', -0.044429007615203658),\n",
       " ('Parch', 0.37165720405974106)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip(features, logisticModel.weights.array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                R2:  -0.720588\n",
      "Explained Variance:  0.192830\n",
      "               MSE:  0.407240\n",
      "              RMSE:  0.638153\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediObserRDD = testData.map(lambda row: (float(logisticModel.predict(row.features)), row.label)).cache()\n",
    "metrics = RegressionMetrics(prediObserRDD)\n",
    "\n",
    "print \"\"\"\n",
    "                R2:  %.6f\n",
    "Explained Variance:  %.6f\n",
    "               MSE:  %.6f\n",
    "              RMSE:  %.6f\n",
    "\"\"\" % (metrics.r2, metrics.explainedVariance, metrics.meanSquaredError, metrics.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 0.407239819005\n",
      "Area under PR = 0.482016205409\n",
      "Area under ROC = 0.523529411765\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Overall accuracy\n",
    "def testError(lap):\n",
    "    return lap.filter(lambda (v, p): v != p).count() / float(testData.count())\n",
    "    \n",
    "accuracy = testError(prediObserRDD)\n",
    "\n",
    "print \"Test Accuracy = %s\" % accuracy\n",
    "\n",
    "# Instantiate metrics object\n",
    "metrics = BinaryClassificationMetrics(prediObserRDD)\n",
    "\n",
    "# Area under precision-recall curve\n",
    "print \"Area under PR = %s\" % metrics.areaUnderPR\n",
    "\n",
    "# Area under ROC curve\n",
    "print \"Area under ROC = %s\" % metrics.areaUnderROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Class Metrics\n",
    "If we had a multinomial response, we could use the _Multiclassmetrics_ to get a more accurate sense of these. It appears that the Scala and Java flavors include these in the binomial version of the metrics but not the Python flavor.  This is here mainly as a reference point for future problems you may want to use.\n",
    "\n",
    "> These metrics are highly suspect.  Only an example of the implementation with Pyspark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Stats\n",
      "--------------------\n",
      "Accuracy  = 0.592760180995\n",
      "Precision = 0.592760180995\n",
      "Recall    = 0.592760180995\n",
      "F1 Score  = 0.592760180995\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "metrics = MulticlassMetrics(prediObserRDD)\n",
    "\n",
    "precision = metrics.precision()\n",
    "recall = metrics.recall()\n",
    "f1Score = metrics.fMeasure()\n",
    "\n",
    "print \"Summary Stats\" \n",
    "print \"--------------------\"\n",
    "print \"Accuracy  = %s\" % metrics.accuracy\n",
    "print \"Precision = %s\" % precision \n",
    "print \"Recall    = %s\" % recall \n",
    "print \"F1 Score  = %s\" % f1Score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests in Pyspark\n",
    "\n",
    "This is a very basic setup for RF's in Pyspark.  One thing that is missing at the time of the writing of this content is that \"feature importances\" aren't implemented inside of the Python library for spark yet, but if you implement these models in Scala or Java, that metric is available as part of the API.  It's very well possible that this feature is available inside the API for Python, but it isn't documented.  Python features usually are implemented last in the chain of the Spark ecosystem.\n",
    "\n",
    "You may recall a rant on Scala vs Python within the Spark ecosystem.  This sould give you a sense about the value of learning Scala or Java for big data.  The best way to learn Spark, is to re-implement what we've done in class using Spark.  Some tasks are much easier, but overall it's a little slower to implement than using Pandas + Sklearn because you are unfamilliar with the Mlib / Spark stack. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.330316742081\n",
      "Learned classification forest model:\n",
      "TreeEnsembleModel classifier with 3 trees\n",
      "\n",
      "  Tree 0:\n",
      "    If (feature 2 <= 3.0)\n",
      "     If (feature 0 <= 2.0)\n",
      "      If (feature 3 <= 0.0)\n",
      "       If (feature 0 <= 1.0)\n",
      "        Predict: 1.0\n",
      "       Else (feature 0 > 1.0)\n",
      "        Predict: 0.0\n",
      "      Else (feature 3 > 0.0)\n",
      "       If (feature 1 <= 47.0)\n",
      "        Predict: 1.0\n",
      "       Else (feature 1 > 47.0)\n",
      "        Predict: 0.0\n",
      "     Else (feature 0 > 2.0)\n",
      "      If (feature 1 <= 16.0)\n",
      "       If (feature 2 <= 2.0)\n",
      "        Predict: 1.0\n",
      "       Else (feature 2 > 2.0)\n",
      "        Predict: 0.0\n",
      "      Else (feature 1 > 16.0)\n",
      "       If (feature 3 <= 1.0)\n",
      "        Predict: 0.0\n",
      "       Else (feature 3 > 1.0)\n",
      "        Predict: 0.0\n",
      "    Else (feature 2 > 3.0)\n",
      "     Predict: 0.0\n",
      "  Tree 1:\n",
      "    If (feature 3 <= 0.0)\n",
      "     If (feature 2 <= 0.0)\n",
      "      If (feature 1 <= 25.0)\n",
      "       If (feature 0 <= 1.0)\n",
      "        Predict: 1.0\n",
      "       Else (feature 0 > 1.0)\n",
      "        Predict: 0.0\n",
      "      Else (feature 1 > 25.0)\n",
      "       If (feature 1 <= 27.0)\n",
      "        Predict: 1.0\n",
      "       Else (feature 1 > 27.0)\n",
      "        Predict: 0.0\n",
      "     Else (feature 2 > 0.0)\n",
      "      If (feature 1 <= 47.0)\n",
      "       If (feature 0 <= 2.0)\n",
      "        Predict: 1.0\n",
      "       Else (feature 0 > 2.0)\n",
      "        Predict: 0.0\n",
      "      Else (feature 1 > 47.0)\n",
      "       If (feature 1 <= 49.0)\n",
      "        Predict: 1.0\n",
      "       Else (feature 1 > 49.0)\n",
      "        Predict: 1.0\n",
      "    Else (feature 3 > 0.0)\n",
      "     If (feature 0 <= 2.0)\n",
      "      If (feature 1 <= 60.0)\n",
      "       If (feature 0 <= 1.0)\n",
      "        Predict: 1.0\n",
      "       Else (feature 0 > 1.0)\n",
      "        Predict: 1.0\n",
      "      Else (feature 1 > 60.0)\n",
      "       Predict: 0.0\n",
      "     Else (feature 0 > 2.0)\n",
      "      If (feature 1 <= 27.0)\n",
      "       If (feature 2 <= 1.0)\n",
      "        Predict: 1.0\n",
      "       Else (feature 2 > 1.0)\n",
      "        Predict: 0.0\n",
      "      Else (feature 1 > 27.0)\n",
      "       If (feature 3 <= 1.0)\n",
      "        Predict: 0.0\n",
      "       Else (feature 3 > 1.0)\n",
      "        Predict: 0.0\n",
      "  Tree 2:\n",
      "    If (feature 0 <= 2.0)\n",
      "     If (feature 0 <= 1.0)\n",
      "      If (feature 1 <= 60.0)\n",
      "       If (feature 1 <= 39.0)\n",
      "        Predict: 1.0\n",
      "       Else (feature 1 > 39.0)\n",
      "        Predict: 1.0\n",
      "      Else (feature 1 > 60.0)\n",
      "       Predict: 0.0\n",
      "     Else (feature 0 > 1.0)\n",
      "      If (feature 3 <= 0.0)\n",
      "       If (feature 1 <= 44.0)\n",
      "        Predict: 0.0\n",
      "       Else (feature 1 > 44.0)\n",
      "        Predict: 0.0\n",
      "      Else (feature 3 > 0.0)\n",
      "       If (feature 3 <= 1.0)\n",
      "        Predict: 1.0\n",
      "       Else (feature 3 > 1.0)\n",
      "        Predict: 1.0\n",
      "    Else (feature 0 > 2.0)\n",
      "     If (feature 1 <= 34.0)\n",
      "      If (feature 3 <= 0.0)\n",
      "       If (feature 1 <= 16.0)\n",
      "        Predict: 0.0\n",
      "       Else (feature 1 > 16.0)\n",
      "        Predict: 0.0\n",
      "      Else (feature 3 > 0.0)\n",
      "       If (feature 2 <= 0.0)\n",
      "        Predict: 1.0\n",
      "       Else (feature 2 > 0.0)\n",
      "        Predict: 0.0\n",
      "     Else (feature 1 > 34.0)\n",
      "      If (feature 2 <= 0.0)\n",
      "       If (feature 3 <= 0.0)\n",
      "        Predict: 0.0\n",
      "       Else (feature 3 > 0.0)\n",
      "        Predict: 0.0\n",
      "      Else (feature 2 > 0.0)\n",
      "       Predict: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "model = RandomForest.trainClassifier(trainingData, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                     numTrees=3, featureSubsetStrategy=\"auto\",\n",
    "                                     impurity='gini', maxDepth=4, maxBins=32)\n",
    "\n",
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(lambda (v, p): v != p).count() / float(testData.count())\n",
    "\n",
    "print('Test Error = ' + str(testErr))\n",
    "print('Learned classification forest model:')\n",
    "print(model.toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Else!?\n",
    "\n",
    "This is barely scratching the surface of what's possible with Spark + Python.  Some models are implemented in MLib that are not in Scikit-learn but Scikit-learn is considered to be a more robust toolset in terms of analsysis on a single machine, however, there are exceptions to that statement.  Mainly, Spark requires a little more attention to types, and preprocessing can be a bit more inolved, but the fact that you can quickly iterate on a machine learning and data processing pipeline, is still a great asset when building predictive models.\n",
    "\n",
    "Some noteable features:\n",
    "\n",
    "- Pipelines\n",
    "- ParamGridSearch\n",
    "- Model Loading / Saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independent Practice\n",
    "\n",
    "Load up the merged version of the wine dataset and attempt to build an entire analysis pipeline with schema, test / train split, model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(\n",
    "    \"../../../datasets/wine_quality/winequality_merged.csv\", header=True, mode=\"DROPMALFORMED\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt linear regression on one of the features as the response!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [dsi]",
   "language": "python",
   "name": "Python [dsi]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
