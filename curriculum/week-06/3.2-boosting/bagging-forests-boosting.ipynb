{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Popular bagging methods and Boosting\n",
    "\n",
    "---\n",
    "\n",
    "- Random Forests\n",
    "- Extremely Randomized Trees\n",
    "- Intro to \"Boosting\" ensemble method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![rf](./images/randomforests_viz.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature Bagging\n",
    "\n",
    "---\n",
    "\n",
    "In a bagging ensemble of decision trees where each tree gets all the predictors, **decision tree base estimators can end up correlated**. \n",
    "\n",
    "For example, if one or a few features are very strong predictors for the dependent variable, these features will be selected in many or most of the bagging base trees. This makes the decisions of the base estimator trees correlated.\n",
    "\n",
    "By selecting a random subset of the features at each split (feature bagging), we reduce the amount of correlation between the base models, which strengthens the predictions of the overall model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature bagging heuristics\n",
    "\n",
    "---\n",
    "\n",
    "Let $p$ be the number of features.\n",
    "\n",
    "**For classification problems**, $\\sqrt{p}$ (rounded down) is the recommended number of features to randomly subset for each split. \n",
    "\n",
    "**For regression problems** $p/3$ (rounded down) is the recommended feature subset size with a minimum node depth of 5 as the default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Extremely Randomized Trees\n",
    "\n",
    "---\n",
    "\n",
    "**Extremely Randomized Trees** (in sklearn: **`ExtraTrees`**) are different from Random Forests in two ways:\n",
    "\n",
    "1. The data sent to each base estimator is no longer a bootstrapped sample.\n",
    "2. Within each base estimator, the node splits are no longer optimized based on purity, but instead split criteria is determined at random.\n",
    "\n",
    "---\n",
    "\n",
    "In other words, **the top-down splitting in each base tree learner is randomized.**\n",
    "\n",
    "The split values at nodes are selected from the feature's possible range in the data to ensure the split actually splits up the data at each node.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Extremely Randomized Trees vs. Random Forests\n",
    "\n",
    "---\n",
    "\n",
    "1. **Reduces variance more than RF:** greater variety of leaf nodes.\n",
    "2. **Bias is increased more than RF**: each base estimator tree has greater expected error between observations and decision boundary.\n",
    "3. **Computationally faster to train**: no optimal split calculations at nodes.\n",
    "4. **Less correlation between base estimator decisions.**\n",
    "5. **Potentially better performance than RF.**\n",
    "\n",
    "---\n",
    "\n",
    "They are not guaranteed to perform better than Random Forests. It depends on the data and the problem. \n",
    "\n",
    "[For a full overview of differences see this paper.](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.65.7485&rep=rep1&type=pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Boosting\n",
    "\n",
    "Boosting is another ensemble method with a theoretically different approach than bagging.\n",
    "\n",
    "---\n",
    "\n",
    "**BOOSTING**\n",
    "\n",
    "1. **Base model fitting an iterative procedure**: it cannot be run in parallel.\n",
    "- **Weights assigned to observations indicating their \"importance\"**: samples with higher weights are given higher influence on the total error of the next model, prioritizing those observations.\n",
    "- **Weights change at each iteration with the goal of correcting the errors/misclassifications of the previous iteration**: the first base estimator is fit with uniform weights on the observations.\n",
    "- **Final prediction is typically constructed by a weighted vote**: weights for each base model depends on their training errors or misclassification rates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pros and cons\n",
    "\n",
    "---\n",
    "\n",
    "**PROS**\n",
    "\n",
    "- Achieves higher performance than bagging when hyper-parameters tuned properly.\n",
    "- Can be used for classification and regression equally well.\n",
    "- Easily handles mixed data types.\n",
    "- Can use \"robust\" loss functions that make the model resistant to outliers.\n",
    "\n",
    "---\n",
    "\n",
    "**CONS**\n",
    "\n",
    "- Difficult and time consuming to properly tune hyper-parameters.\n",
    "- Cannot be parallelized like bagging (bad scalability when huge amounts of data).\n",
    "- More risk of overfitting compared to bagging.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![boostvsbag](./images/BoostingVSBagging.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Boosting and bias-variance \n",
    "\n",
    "---\n",
    "\n",
    "Recall that **bagging aims to reduce variance**.\n",
    "\n",
    "**Boosting aims to reduce bias!** (and can reduce variance a bit as well).\n",
    "\n",
    "---\n",
    "\n",
    "#### Why?\n",
    "\n",
    "The rationale/theory behind Boosting is to combines **many weak learners into a single strong learner.**\n",
    "\n",
    "Instead of deep/full decision trees like in bagging, **Boosting uses shallow/high bias base estimators.**\n",
    "\n",
    "Thus, each weak learner has:\n",
    "\n",
    "- Low variance\n",
    "- High bias\n",
    "\n",
    "And the iterative fitting to explain error/misclassification unexplained by the previous base models reduces bias without increasing variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### AdaBoost\n",
    "\n",
    "---\n",
    "\n",
    "AdaBoost is the original boosting algorithm. Predictions from AdaBoost follow the formula:\n",
    "\n",
    "\n",
    "### $$ AdaBoost(X) = sign\\left(\\sum_{t=1}^T\\alpha_t h_t(X)\\right) $$\n",
    "\n",
    "where\n",
    "\n",
    "$AdaBoost(X)$ is the classification predictions for $y$ using predictor matrix $X$\n",
    "\n",
    "$T$ is the set of \"weak learners\"\n",
    "\n",
    "$\\alpha_t$ is the contribution weight for weak learner $t$\n",
    "\n",
    "$h_t(X)$ is the prediction of weak learner $t$\n",
    "\n",
    "and $y$ is binary **with values -1 and 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The weak learner classifiers are trained sequentially.  After each fit, the importance weights on each observation need to be updated. AdaBoost, like all boosting ensemble methods, focuses the next model's fit on the misclassifications/weaknesses of the prior models.\n",
    "\n",
    "All training examples start with equal importance weighting. When we finish training a classifier, we update the importance weighting of the classifier itself represented by alpha $\\alpha$.\n",
    "\n",
    "### $$ \\alpha_t = \\frac{1}{2}ln \\left(\\frac{1-\\epsilon_t}{\\epsilon_t}\\right)$$\n",
    "\n",
    "Where $\\epsilon_t$ is the misclassification rate for the current classifier:\n",
    "\n",
    "### $$ \\epsilon_t = \\frac{\\text{misclassifications}_t}{\\text{observations}_t} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Training sample importance weights are adjusted according to this alpha parameter. First update the weights like so:\n",
    "\n",
    "### $$ D_{t+1}(i) = D_t(i) e^{-\\alpha_t y_i h_t(x_i)} $$\n",
    "\n",
    "And then each by the sum of weights to normalize them, ensuring that they sum to 1:\n",
    "\n",
    "### $$ D_{t+1}(i) = \\frac{D_{t+1}(i)}{\\sum_{i=1}^N D_{t+1}(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient Boosting Models\n",
    "\n",
    "---\n",
    "\n",
    "A Gradient Boosting Model (GBM) is a generalization of boosting that is essentially an **approximation of gradient descent**.\n",
    "\n",
    "---\n",
    "\n",
    "### Why?\n",
    "\n",
    "**In gradient descent, we minimized prediction error with regard to the coefficients $b_1 ... b_i$**\n",
    "\n",
    "**GBM minimizes with respect to the function defining prediction errors $f(x)$**\n",
    "\n",
    "More intuitively, at each step in the GBM:\n",
    "- A model $h(x)$ is constructed to further reduce overall error defined by $f(x)$\n",
    "- The model $h(x)$ therefore _emulates a step descending the gradient of the total error space._ \n",
    "\n",
    "By minimizing the error with respect to the function we can perform the \"gradient descent\" down a loss function like least-squares loss for non-parametric models!\n",
    "\n",
    "---\n",
    "\n",
    "_For more math-y explanations (as if this wasn't bad enough) see [here](https://www.quora.com/What-is-an-intuitive-explanation-of-Gradient-Boosting) and [here](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3885826/)_\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
