{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a basic Random Forest\n",
    "\n",
    "The Random Forest model is a popular bagging ensemble method. It combines many decision tree classifiers or regressors as the \"base models\" to make predictions.\n",
    "\n",
    "By building this ourselves we will get to see the internals of exactly what is going on in a bagging ensemble model.\n",
    "\n",
    "---\n",
    "\n",
    "### Construction of the RF\n",
    "\n",
    "The Random Forest classifier is built such that:\n",
    "\n",
    "1. Multiple internal decision tree classifiers will be built as the base models\n",
    "- For each base model, the data will be resampled like in bootstrapping.\n",
    "- Each decision tree will be fit on the bootstrapped sample of the data.\n",
    "- To predict, each internal base model will be passed the new data and make their predictions. The final output will be a vote across the base models for the class.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic = pd.read_csv('/Users/kiefer/github-repos/DSI-SF-2/datasets/titanic/titanic_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This will be the custom, basic version of a Random Forest in the style of\n",
    "# sklearn's models\n",
    "class RandomForest(object):\n",
    "    \n",
    "    # The __init__ function takes 3 keyword arguments:\n",
    "    # n_estimators: how many decision tree classifiers are we going to \n",
    "    # fit?\n",
    "    # max_depth: what is the max depth of the internal \"base estimator\"\n",
    "    # decision trees?\n",
    "    # max_features: also a parameter passed to the decision tree classifier\n",
    "    def __init__(self, n_estimators=3, max_depth=None, max_features=None):\n",
    "        \n",
    "        # Set up the 3 keyword arguments as class attributes:\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        \n",
    "        # create a dictionary that will hold our base estimators\n",
    "        self.base_estimators = {}\n",
    "        \n",
    "        \n",
    "    # The makes the internal decision tree classifiers\n",
    "    # Takes data X, y and also an estimator number for\n",
    "    # assignment to the self.base_estimators dictionary\n",
    "    def _make_base_estimator(self, X, y, estimator_number):\n",
    "        \n",
    "        # make random sample of X, y\n",
    "        row_indices = range(X.shape[0])\n",
    "        \n",
    "        # np.random.choice gives us a random sample of the row\n",
    "        # indice with replacement\n",
    "        random_indices = np.random.choice(row_indices, size=len(row_indices),\n",
    "                                          replace=True)\n",
    "        \n",
    "        \n",
    "        # make \"bootstrapped\" X, y\n",
    "        # versions of the data indexed by the randomly sampled row indices\n",
    "        Xr = X.iloc[random_indices, :]\n",
    "        yr = y[random_indices]\n",
    "        \n",
    "        # Initialize a decision tree classifier with the max_depth\n",
    "        # and max_features attributes\n",
    "        dtc = DecisionTreeClassifier(max_depth=self.max_depth,\n",
    "                                     max_features=self.max_features)\n",
    "        \n",
    "        # fit the dtc:\n",
    "        dtc.fit(Xr, yr)\n",
    "        \n",
    "        # put the dtc into our base_estimator dictionary\n",
    "        self.base_estimators[estimator_number] = dtc\n",
    "                \n",
    "    \n",
    "    # Fitting is actually delegated to the _make_base_estimator function.\n",
    "    # You specify a number of estimators that you want to fit, and so\n",
    "    # it iterates through that range, fitting that number of trees\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            self._make_base_estimator(X, y, i)\n",
    "            \n",
    "    \n",
    "    # Predict calls back into the stored base estimators to get\n",
    "    # predictions for each. \n",
    "    # NOTE: THIS FUNCTION ONLY WORKS FOR THE BINARY 1/0 CLASS\n",
    "    # PROBLEM!\n",
    "    def predict(self, X):\n",
    "        \n",
    "        # set up votes array with just zeros in it, \n",
    "        # the length of rows in X:\n",
    "        votes = np.zeros(X.shape[0])\n",
    "        \n",
    "        # iterate through all the base estimators\n",
    "        for i in range(self.n_estimators):\n",
    "            # get out the current one from the dictionary\n",
    "            base_model = self.base_estimators[i]\n",
    "            \n",
    "            # predict with the base estimator on the X matrix\n",
    "            current_pred = base_model.predict(X)\n",
    "            \n",
    "            # add whatever the predictions are to the votes array\n",
    "            votes = votes + current_pred\n",
    "            \n",
    "        # The final output predictions will be 1 any time the vote \n",
    "        # count for class 1 was greater than half the number of \n",
    "        # base estimators. \n",
    "        yhat = (votes >= self.n_estimators/2.).astype(np.int)\n",
    "        \n",
    "        return yhat\n",
    "        \n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \n",
    "        y = np.array(y)\n",
    "        \n",
    "        yhat = self.predict(X)\n",
    "        \n",
    "        return ()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import patsy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y, X = patsy.dmatrices('Survived ~ C(Pclass) + C(Sex) + Age + SibSp + Parch + Fare + C(Embarked) -1',\n",
    "                      data=titanic, return_type='dataframe')\n",
    "y = np.ravel(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4044943820224719"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtc score 0.736170212766\n",
      "rf score 0.804255319149\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForest(n_estimators=1000, max_depth=None, max_features='auto')\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "dtc = DecisionTreeClassifier(max_depth=None, max_features='auto')\n",
    "dtc.fit(Xtrain, ytrain)\n",
    "print 'dtc acc:', dtc.score(Xtest, ytest)\n",
    "\n",
    "rf.fit(Xtrain, ytrain)\n",
    "yhat = rf.predict(Xtest)\n",
    "print 'rf acc:', accuracy_score(ytest, yhat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
