{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Intro to ensemble methods\n",
    "\n",
    "**Ensemble methods** are supervized learning models which combine the predictions of multiple smaller models to improve predictive power and generalization.\n",
    "\n",
    "The smaller models that combine to make the ensemble model are referred to as **base models**. Ensemble methods often result in considerably higher performance than any of the individual base models could achieve.\n",
    "\n",
    "![ensemble](./images/Ensemble.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## When to use ensembles\n",
    "\n",
    "    - Medical diagnoses\n",
    "    - Predicting disease outbreak, natrual disasters\n",
    "    - Stock market predictions\n",
    "    - AI\n",
    "\n",
    "Or any case where the highest performance is desired at the expense of model interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Two popular families of ensemble methods\n",
    "\n",
    "---\n",
    "\n",
    "**BAGGING**\n",
    "\n",
    "Several estimators are built independently on subsets of the data and their predictions are averaged. Typically the combined estimator is usually better than any of the single base estimator.\n",
    "\n",
    "**Bagging can reduce variance with little to no effect on bias.**\n",
    "\n",
    "    ex: Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "---\n",
    "\n",
    "**BOOSTING**\n",
    "\n",
    "Base estimators are built sequentially. Each subsequent estimator focuses on the weaknesses of the previous estimators. In essence several weak models \"team up\" to produce a powerful ensemble model. (We will discuss these later this week.)\n",
    "\n",
    "**Boosting can reduce bias without incurring higher variance.**\n",
    "\n",
    "    ex: Gradient Boosted Trees, AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Potential deficiencies of base models\n",
    "\n",
    "There are three categories of weaknesses in which \"base models\" can fail or produce poor results:\n",
    "\n",
    "1. Statistical problems\n",
    "2. Computational problems\n",
    "3. \"Representational\" problems\n",
    "\n",
    "Ensemble methods are designed to address any or all three.\n",
    "\n",
    "---\n",
    "\n",
    "Let\n",
    "\n",
    "### $$ \\begin{aligned} \\text{true function of data} &= f() \\\\ \\text{model function of data} &= h() \\end{aligned}$$\n",
    "\n",
    "Where $h()$ can be a classifier or a regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Statistical problem\n",
    "\n",
    "**The amount of training data available is small**. A single base classifier will have difficulty converging to $h()$.\n",
    "\n",
    "![statistical](./images/statistical.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "---\n",
    "\n",
    "A bagging ensemble model, for example, mitigates this problem by \"averaging out\" base classifier predictions to improve convergence on the true function.\n",
    "\n",
    "[Paper describing in-depth reason for this.](http://web.cs.iastate.edu/~jtian/cs573/Papers/Dietterich-ensemble-00.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Computational problem\n",
    "\n",
    "There is sufficient training data, but it is computationally intractable to find the best model $h()$.\n",
    "\n",
    "For example, if a base classifier is a decision tree, an exhaustive search of the hypothesis space of all possible classifiers is extremely complex (NP-complete).\n",
    "\n",
    "This is, for example, why decision trees use heuristic algorithms at nodes (greedy search).\n",
    "\n",
    "![computational](./images/computational.png)\n",
    "\n",
    "---\n",
    "\n",
    "Ensembles composed of several, simpler base models using different starting points can converge faster to a good approximation of $f()$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Representational problem\n",
    "\n",
    "Suppose we use a decision tree as a base classifier. A decision tree works by forming a \"rectilinear\" partition of the feature space, **i.e it always cuts at a fixed value along a feature.**\n",
    "\n",
    "But what if $f()$ is best modeled by diagonal line?\n",
    "\n",
    "It cannot be represented by a finite number of rectilinear segments, and the true decision boundary cannot be obtained by the decision tree classifier.\n",
    "\n",
    "![dtcut](./images/dtcut.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**A representational problem occurs when $f()$ cannot be expressed in terms of our hypothesis at all.** \n",
    "\n",
    "Yet, it may be still be possible to approximate $f()$ by expanding the space of representable functions using ensemble methods!\n",
    "\n",
    "![representational](./images/representational.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conditions for ensembles to outperform base models\n",
    "\n",
    "For an ensemble method to perform better than a base classifier, it must meet these two criteria:\n",
    "\n",
    "1. **Accuracy:** the combination of base classifiers must outperform random guessing. \n",
    "2. **Diversity:** base models must not be identical in classification/regression estimates.\n",
    "    - [Description of diversity.](https://www.cs.cmu.edu/afs/cs/project/jair/pub/volume11/opitz99a-html/node2.html)\n",
    "    - [Paper on measures of diversity.](http://staff.ustc.edu.cn/~ketang/papers/TangSuganYao_ML06.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bagging\n",
    "\n",
    "The ensemble method we will be using today is called **bagging**, which is short for **bootstrap aggregating**.\n",
    "\n",
    "Bagging builds multiple base models with **resampled training data with replacement.** We train $k$ base classifiers on $k$ different samples of training data. Using random subsets of the data to train base models promotes more differences between the base models.\n",
    "\n",
    "Random Forests, which \"bag\" decision trees, can achieve very high classification accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bagging's magic decrease of model variance \n",
    "\n",
    "One of the biggest advantages of Random Forests is that they **decrease variance without increasing bias**. Essentially you can get a better model without having to trade off between bias and variance.\n",
    "\n",
    "---\n",
    "\n",
    "**VARIANCE DECREASE**\n",
    "\n",
    "Base model estimates are averaged together, so variability of model predictions (across hypothetical samples) is lower.\n",
    "\n",
    "---\n",
    "\n",
    "**NO/LITTLE BIAS INCREASE**\n",
    "\n",
    "The bias remains the same as the bias of the individual base models. The model is still able to model the \"true function\" since the  base models' complexity is unrestricted (low bias).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [dsi]",
   "language": "python",
   "name": "Python [dsi]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
