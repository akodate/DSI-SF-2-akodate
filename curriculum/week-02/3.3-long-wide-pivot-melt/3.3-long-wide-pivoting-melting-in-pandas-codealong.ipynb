{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long format, wide format, pivot tables, and melting\n",
    "\n",
    "This lesson is all about data transformation in pandas. Data transformation is in essense reorganizing the rows and columns of your dataset to be a different shape and format. \n",
    "\n",
    "The benefits to transforming your data are primarily for easier access and manipulation of data, whether it be through easier masking/conditional statements or because you would prefer to operate across columns or down rows. \n",
    "\n",
    "Over time you will get a feel for which data formats are better for different tasks. This lesson, however, is focused in large part on the _functional application_ of data transformation. How do you do this to a dataset?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. \"Wide\" format data\n",
    "\n",
    "**Wide** format data is the more common format of data for .csv type files. You are already familiar with wide format data: I believe all of the datasets we have been using thus far have been in wide format.\n",
    "\n",
    "Wide format data is formatted with criteria:\n",
    "\n",
    "- There are multiple ID _and_ value columns. In other words, there is a column for every \"variable\" with its own unique values.\n",
    "- The format has both the conceptual simplicity of a single column of values per variable and a more compact matrix.\n",
    "- Is not useful for SQL-style operations: it can make it much harder or even impossible to join tables together on a value.\n",
    "- Can be more useful in pandas when you need to preform operations on variables **across columns**. For example, multiplying columns together.\n",
    "- It is the most commonly the format that you will put the data in when you are ready to perform modeling (with some exceptions). When we get into modeling next week I will explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Load  \"Nerdy Personality Attributes\" dataset\n",
    "\n",
    "This is a parsed and modified version of the full \"Nerdy Personality Attributes\" survey that asked subjects to self-rate on questions related to \"nerdiness\" as well as more general personality traits such as openness and extraversion. Demographic information on the subjects was also collected.\n",
    "\n",
    "In this modified version, for the sake of example, some of the subjects have only data for the survey and not the demographic variables. Because there are missing values and the data in general is \"messy\", this is also in part a data cleaning problem.\n",
    "\n",
    "We will load the data in wide format first:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nerdy_wide_f = '~/github-repos/DSI-SF-2/datasets/nerdy_personality_attributes/NPAS_parsed_trunc_wide_missing.csv'\n",
    "\n",
    "#nerdy_wide = pd.read_csv(nerdy_wide_f)\n",
    "#print nerdy_wide.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is in the familiar (rows, columns) format where each column is a variable, each row contains the observation for that variable for (in this case) that distinct subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#nerdy_wide.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check to see how many null values there are per column with the convenient chained function pattern below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 691 missing demographic variables are intentional (I specifically set it up so only 700 of the subjects have demographic information).\n",
    "\n",
    "However, we can see that the `major` variable has 970 missing values. This is not intentional by me.\n",
    "\n",
    "If we were to just drop all the rows that have any null values at this point, we would lose 970 rows due to the commonly missing variable `major`.\n",
    "\n",
    "With a numeric column, this would be hard to avoid without \"imputing\" some number to fill in the values. In the simplest case imputing the mean or median for missing numeric values is used (but not very good).\n",
    "\n",
    "With a **categorical variable**, which `major` is, we have a luxury of replacing the missing values with another category. In this case, I will replace the values with \"unknown\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. \"Long\" format\n",
    "\n",
    "Now we can load the same data in but in what's commonly referred to as \"long format\". \n",
    "\n",
    "Long data is formatted with criteria:\n",
    "\n",
    "- Potentially multiple \"id\" (identification) columns.\n",
    "- Variable:value column pairs that match a variable key to a value (in the simple case, a single variable column and a single value column).\n",
    "- The \"variable\" column corresponds to the multiple variable columns in your wide format data. Now, instead of a column for each variable, you have a row for each variable:value pair, per id. \n",
    "- This is a standard format in SQL databases because it is appropriate for joining different tables together by keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nerdy_long_f = '~/github-repos/DSI-SF-2/datasets/nerdy_personality_attributes/NPAS_parsed_trunc_long_missing.csv'\n",
    "\n",
    "#nerdy_long = pd.read_csv(nerdy_long_f)\n",
    "#print nerdy_long.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the long data has way more rows, but only three columns.\n",
    "\n",
    "Below you see the three columns: `subject_id`, `variable`, and `value`.\n",
    "\n",
    "**`subject_id:`**\n",
    "- This is the primary \"key\" or \"id\" column. Each subject id will have corresponding entries in the variable column, one for each row.\n",
    "\n",
    "**`variable:`**\n",
    "- This column indicates which variable the item in the value column corresponds to.\n",
    "\n",
    "**`value:`**\n",
    "\n",
    "- This contains all the values for all of the variables for all ids. Essentially, every cell in the wide dataset except the subject_id is listed in this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#nerdy_long.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the unique values in the variable column correspond to the column headers in the wide format data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's again replace the `major` variables with 'unknown', but in a way that works with long format data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas `pivot_table()`: long to wide format\n",
    "\n",
    "The `pd.pivot_table()` function is a very powerful tool to both transform data from long to wide format and also to conveniently summarize data into a matrix with arbitrary functions.\n",
    "\n",
    "First we'll look at how we transform this long format data back into the wide format data.\n",
    "\n",
    "**Parameters to note in the function:**\n",
    "\n",
    "    nerdy_long: the pivot_table() function takes a dataframe to pivot as its first argument\n",
    "    \n",
    "- **`columns`**: this is the list of columns in the wide format data to transform back to columns in wide format, with each unique value in the long format column becoming a header for the wide format   \n",
    "- **`values`**: a single column indicating the values to use when pivoting and filling in the new wide format columns\n",
    "- **`index`**: columns in the long format data that are index variables â€“ this means that these will be left as single columns, not spread out across columns by unique value such as in the columns parameter \n",
    "- **`aggfunc`**: often pivot_table() is used to perform a summary of the data. aggfunc stands for \"aggregation function\". It is required and defaults to np.mean. You can put your own function in, which I do below.\n",
    "- **`fill_value`**: if a cell is missing for the wide format data, the value to fill in\n",
    "    \n",
    "I am putting in my own function, `select_item_or_nan()` to the `aggfunc` keyword argument. Because my `subject_id` column has a single variable value for each id, I just want the single element in the long format value cell. My data is messy and so I have to write a function to check for some places it can break. \n",
    "\n",
    "Note: `x` passed into my function is a series object (weirdly). I pull out the first element of that with the `.iloc` indexer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiindex/Hierarchical indexing pt. 1\n",
    "\n",
    "Below in the header you can see that the format of the wide data is not the same as our original loaded wide format. Pandas implements something called **Multiindexing** or **Hierarchical indexing** which allows for \"tiered\" row and column labels.\n",
    "\n",
    "Right now it is not that bad, but this can get very complicated and annoying which we will see further down in the lesson.\n",
    "\n",
    "The main difference here is that we have a `variable` name in the top left corner, which is \"labeling\" our columns (and corresponds to the name of our original column in the long format data). The row indexer has become our single key/id variable `subject_id`. The columns are what we would expect here, each one a variable like in the original wide data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop the null values from our recreated wide data.\n",
    "\n",
    "Remember our `subject_id` is now the **index**, and so we can access it with the `.index` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the dataframe function `.reset_index()` to move `subject_id` into a column and create a new index. Now we have the dataframe in the format we got when we loaded the original wide data in before. The only exception is that we still have that \"variable\" column label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can remove the column label (which I personally find confusing) by setting the `.columns.name` attribute to None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `pivot_table` for summarization\n",
    "\n",
    "For those of you who are experienced with Excel, the pandas pivot table does the same thing as the pivot table in Excel. It's more powerful, but obviously harder to use than the user-friendly spreadsheet version.\n",
    "\n",
    "Next we'll use pivot table to generate some summary statistics for `anxious`, `bookish`, and `calm` by `major`. \n",
    "\n",
    "We can do it two ways. First let's subset the data just to those columns and subject id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going from wide to long with `.melt()`\n",
    "\n",
    "**`.melt()`** is a function that essentially performs the inverse operation of `pivot_table` on dataframes.\n",
    "\n",
    "Melt takes a dataframe as its first argument. Additional arguments typically used in the melt function are:\n",
    "\n",
    "- **`id_vars`**: the column or columns that will be id variables. id variables contain datapoints specified by the variable and value columns\n",
    "- **`value_vars`**: a list that specifies which columns should be converted into a single value column and variable column.\n",
    "- **`var_name`**: the header name of the variable column (default='variable')\n",
    "- **`value_name`**: the header name of the value column (default='value')\n",
    "\n",
    "Below I only specify the `id_vars` as subject_id and major. The variable and value columns are inferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do the same thing as above without having to subset the dataframe first by simply specifying the value_vars to lengthen. The output dataframe will then not have information on the columns left out of the `id_vars` and `value_vars` arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value column is still a string, so we can convert it to float:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing with aggregate functions\n",
    "\n",
    "Pivot table can take in the long format variable, value, and an index to group by and apply aggregate functions as well for summarizing data easily. Note that your index variable should not be pulling out unique rows (for example, subject_id by variable would only have one value to send into the aggregate functions).\n",
    "\n",
    "The output dataframe gives you a \"hierarchical\" column index â€“ the three variable for each aggregate function. The row index is the majors you divided the data up by.\n",
    "\n",
    "If you apply more index variables to split by, the row indices will also become hierarchical! It can get complicated fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.names` attribute on the index and the columns will show you the hierarchy of labels. The row index is \"major\", and the two column indices are None and 'variable' (the aggregate functions get no label from pivot table in this case). \n",
    "\n",
    "If you print out the columns, you can see it has become a pandas `MultiIndex` object that has levels, labels, and names. I won't go into too much detail on this â€“ reading the pandas documentation on MultiIndexes has a lot more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing along the hierarchical column headers can be done with chained bracket keys, with the top level column label in the first bracket down to the bottom level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases you can just split them up by comma within the brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting a MultiIndex dataframe to \"flat\"\n",
    "\n",
    "Personally, while I see multiindex dataframes as potentiall useful and a cool concept, I think the overhead and confusion on how to subset/mask them is annoying, especially when you have to start doing modeling pulling out data from these DataFrames.\n",
    "\n",
    "To \"flatten\" a multi-indexed dataframe down, you can use the `.to_records()` function. To make this a new dataframe, it needs to be wrapped in a `pd.DataFrame()` like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the new column names are tuples of the hierarchy of the multiindexed columns. You can convert these to new, more easily indexed columns with a list comprehension, for example with the comprehension below.\n",
    "\n",
    "The **eval** function takes a string and trys to evaluate it as if it were a python command! Be careful with this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Preface to merging/joining: long and wide data\n",
    "\n",
    "You will practice merging and joining much more tomorrow, but this section is a preview for what is to come with a focus on the difference between merging long and wide datasets together.\n",
    "\n",
    "Load in the data we've been using above, but now split up with just the demographic variables in one dataset and the survey question answers in another. These datasets are in wide format, and they both contain `subject_id` to identify who the questions are for. \n",
    "\n",
    "As you may recall, the demographic responses have fewer observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_demos_file = '~/github-repos/DSI-SF-2/datasets/nerdy_personality_attributes/NPAS_parsed_trunc_demo_sample.csv'\n",
    "n_survey_file = '~/github-repos/DSI-SF-2/datasets/nerdy_personality_attributes/NPAS_parsed_trunc_survey.csv'\n",
    "\n",
    "#demos_subset = pd.read_csv(n_demos_file)\n",
    "#survey = pd.read_csv(n_survey_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas `.merge()` function\n",
    "\n",
    "The merge function is a built-in function in a DataFrame. The first argument is another DataFrame that you want to merge it with, and the `on` keyword argument is the key or keys that you want the DataFrames to be \"matched\" on.\n",
    "\n",
    "We are specifying `how='inner'` here, which essentially means that the subject_id has to be present in both dataframes to merge them together and return them. Because the demographics dataset has fewer subject_ids, it will only merge the subject_id rows from the survey dataset that are present in the demographics dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the demographic and survey data long format using melt\n",
    "\n",
    "This is the same way we used melt in a previous section. \n",
    "\n",
    "- For the demographic dataframe, specify two id_vars, gender and subject_id.\n",
    "- For the survey dataframe, only specify subject_id for id_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge together the long form datasets just like we did before with the wide format data.\n",
    "\n",
    "Here we will still merge on 'subject_id' with 'inner' for the how variable. We have duplicate named columns in each of these dataframes ('variable' and 'value'). We can specify `suffixes=('_survey','_demo')` to give the instances of the survey and demographic dataframes appropriate column names when they are joined together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot with the merged long dataframe\n",
    "\n",
    "Now, use the pivot_table function on the merged demographics and survey dataframes (the long one) with columns the variable column for survey as well as the variable column for the demographics. Make the values the survey values column and the index gender. Set the aggregate function to just be the mean.\n",
    "\n",
    "For example:\n",
    "\n",
    "```python\n",
    "demo_survey_means = pd.pivot_table(demos_survey_long, columns=['variable_survey', 'variable_demo'], \n",
    "                                   values='value_survey',\n",
    "                                   index=['gender'], aggfunc=[np.mean],\n",
    "                                   fill_value=np.nan)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that if you specify multiple variable columns in the columns argument, it will stack them in a hierarchical column setup. So, for every variable in variable_survey, the mean for each gender for each variable in variable_demo.\n",
    "\n",
    "A simpler version below just has the variable_demo in the columns argument, in which case it calculates the mean across those variables for each gender in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [dsi]",
   "language": "python",
   "name": "Python [dsi]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
