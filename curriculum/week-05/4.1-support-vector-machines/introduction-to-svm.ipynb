{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from ipywidgets import *\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Support vector machines\n",
    "\n",
    "Today we'll be learning about a different approach to classification called Support Vector Machines (SVM).\n",
    "\n",
    "These fit a decision boundary similarly to a regression, but uses a different loss function called the \"hinge loss\" (as opposed to the log loss in logistic regression).\n",
    "\n",
    "SVMs are notorious for being less intuitive than other classifiers such as kNN and Logistic regression, but hopefully you will have a feel for it by the end of the lecture.\n",
    "\n",
    "[For a really great resource check out these slides (many of whch I've taken to put in this lecture).](http://www.robots.ox.ac.uk/~az/lectures/ml/lect2.pdf)\n",
    "\n",
    "[This website is also a great resource, on a slightly more technical level.](http://nlp.stanford.edu/IR-book/html/htmledition/support-vector-machines-the-linearly-separable-case-1.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How does the SVM classify?\n",
    "\n",
    "#### It's important to start with the intuition for the special _linearly separable_ classification case.\n",
    "\n",
    "\n",
    "If classification of observations is \"linearly separable\", SVM fits the **\"decision boundary\"** that is defined by the largest margin between the closest points for each class. This is commonly called the **\"maximum margin hyperplane (MMH)\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![linearly separable SVM](./images/linear_separability_vs_not.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Intuition behind the SVM decision boundary\n",
    "\n",
    "SVM's criterion for a decision surface is one that is _maximally far away from any data point between classes_. The distance from the decision boundary to the closest data point determines the \"margin\" of the classifier.\n",
    "\n",
    "The points SVM uses to fit the decision boundary are called \"support vectors\". This term comes from linear algebra: in a vector space, points can be defined as a vector between the origin and that point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![linearly separable SVM MMH margin](./images/Margin.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Intuition cont.\n",
    "\n",
    "Why maximize the margin? **SVM solves for the decision boundary that minimizes generalization error.** \n",
    "\n",
    "Observations that are near the decision boundary between the classes are the most ambiguous observations. They are the observations that are approaching equal chance to be one class or the other.\n",
    "\n",
    "SVM, instead of considering all the observations \"equally\" in the loss function it minimizes, defines it's fit using the most ambiguous points. It's decision boundary is _safe_ in that errors in new measured observations are not likely to cause the SVM to mis-classify.\n",
    "\n",
    "The SVM is concerned with generalization to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Maximum margin hyperplane function\n",
    "\n",
    "The decision boundary (which is equivalent to the MMH) is derived by the [discriminant function](https://en.wikipedia.org/wiki/Discriminant_function_analysis#Discriminant_functions). a linear combination of predictors that maximizes the difference between groups. \n",
    "\n",
    "You are already familiar with the discriminant function for logistic regression, represented by the sigmmoid curve solved by the log loss. \n",
    "\n",
    "In an SVM, the discriminant function is:\n",
    "\n",
    "### $$ f(x) = sign(w^T x + b) $$\n",
    "\n",
    "where **$w$** is the normalized weight vector, perpendicular to the decision boundary.\n",
    "\n",
    "$b$ is the \"bias\", which is the corollary to the intercept in regression.\n",
    "\n",
    "(Note that the $bias$ term is sometimes called the intercept. Don't confuse it with bias-variance tradeoff we discussed earlier.)\n",
    "\n",
    "The solution to $f(x)$ for an $x$ observation is a sign (-1, or 1), and this determines the (binary-case) class label of the observation $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![linearly separable SVM MMH margin](./images/linear_sep_support_vecs_math.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The hinge loss function\n",
    "\n",
    "The function that the SVM minimizes to find the boundary is:\n",
    "\n",
    "### $$  minimize \\;\\; ||w||^2 + C\\sum_{i=1}^N max\\left(0, 1 - y_i \\: f(x_i)\\right) $$\n",
    "\n",
    "where $||w||^2$ is the magnitude of the weight vector squared\n",
    "\n",
    "$C$ is a regularization term, which we will get to soon. Small $C$ creates a wider margin, and large $C$ creates a tighter margin.\n",
    "\n",
    "### $$\\sum_{i=1}^N max\\left(0, 1 - y_i \\: f(x_i)\\right)$$ \n",
    "\n",
    "is the sum of the errors. \n",
    "\n",
    "If $f(x_i) > 1$, the point lies _outside_ the margin and does not contribute to the loss.\n",
    "\n",
    "If $f(x_i) = 1$ the point is _on_ the margin and does not contribute to the loss.\n",
    "\n",
    "If $f(x_i) < 1$ the point lies _inside_ the margin and **does** contribute to the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Hinge loss](./images/hinge_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![linearly separable SVM MMH margin](./images/linear_sep_support_vecs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hinge loss cont.\n",
    "\n",
    "### $$  minimize \\;\\; ||w||^2 + C\\sum_{i=1}^N max\\left(0, 1 - y_i \\: f(x_i)\\right) $$\n",
    "\n",
    "\n",
    "The right hand side of the function to be optimized is saying:\n",
    "\n",
    "> Every point should be on the correct side of the margins for both classes.\n",
    "    \n",
    "The left hand side is saying:\n",
    "\n",
    "> Maximize the distance between the margins by modifying the weight vector.\n",
    "\n",
    "It seems in the above equation like we are minimizing the margin due to the $||w||^2$ term. This is in fact maximizing the margin, but the math behind it is more involved. [This page, which I linked to before, does a good job explaining it in my opinion.](http://nlp.stanford.edu/IR-book/html/htmledition/support-vector-machines-the-linearly-separable-case-1.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hyper-parameter $C$\n",
    "\n",
    "The hyper-parameter $C$ controls the extent to which the SVM is tolerant to errors. It is sometimes called the \"soft-margin constant\". \n",
    "\n",
    "$C$ affects the strength of the \"penalty\" similar to the lambda terms in the Ridge and Lasso. \n",
    "\n",
    "By multiplying the sum of the errors, which are the distances from the margins to the points inside of the margin, it allows the SVM to classify non-linearly separable problems by allowing errors to occur. \n",
    "\n",
    "The lower the value of $C$, the more misclassified observations errors are allowed. These misclassified points are known as \"slack variables\". Reducing the effect of errors on the loss function puts the emphasis on widening the margin.\n",
    "\n",
    "For those interested in exporing the math more, [there is a good tutorial here.](http://www.svm-tutorial.com/2015/06/svm-understanding-math-part-3/#more-457)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![soft margin](./images/slack_variables.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![soft margin](./images/soft_margin.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solving non-linearly separable problems with the \"kernel trick\"\n",
    "\n",
    "The math behind the \"kernel trick\" is beyond the scope of this lecture, but it allows an SVM to classify non-linearly separable problems. It is a big reason why SVMs are so popular.\n",
    "\n",
    "All you need to understand for now is that with an SVM **you can arbitrarily transform your observations that _have no linear seperability_ by putting them into a different \"dimensional space\".** \n",
    "\n",
    "This is done by \"wrapping\" your predictors in a kernel function that transforms them to a different dimensional space. At the end, they can be transformed back, along with the decision boundary, to their original dimensional space.\n",
    "\n",
    "[Check out these lecture slides for a good overview.](http://www.robots.ox.ac.uk/~az/lectures/ml/lect3.pdf)\n",
    "\n",
    "The following pictures should give you at least a general intuition of what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![kernel transform viz](./images/kernel_trick.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![kernel transform viz](./images/kernel_viz.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![polynomial kernel](./images/nonlinear-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![gaussian kernel](./images/nonlinear-2.png)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
