{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn and stochastic gradient descent\n",
    "\n",
    "Until now we've been using the more standard LassoCV, RidgeCV, etc. (or GridSearchCV) to find our optimal parameters. Unfortunately, though these methods work well on smaller datasets with relatively small numbers of columns, once you start getting into \"Medium Data\" these slow down to a crawl, and take up so much memory that fitting them becomes untenable.\n",
    "\n",
    "This is where stochastic gradient descent comes in. Because of its ability to fit iteratively on portions of the data, it avoids the issue of large datasets. It is the most common algorithm to fit models on large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### Import the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy \n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "import patsy\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Load the data\n",
    "\n",
    "I've provided data from the SF assessor's office on housing prices in San Francisco - already cleaned up. However, if you want to try this out on data you've been having trouble fitting, such as the Yelp data for project 5, feel free!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prop = pd.read_csv('/Users/kiefer/github-repos/DSI-SF-2/datasets/sf_assessor_value/assessor_value_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Sample down the data\n",
    "\n",
    "For demonstration purposes and the sake of speed, I am sampling down this large dataset to a more reasonable amount of rows. \n",
    "\n",
    "Stochastic gradient descent is much faster, but the real benefit in my opinion is that it can fit much larger datasets. That being said, it is still slow to fit on huge datasets in sklearn. I don't recommend fitting on the entire data. This is actually my recommendation in general; finding the optimal parameters with more and more data will change the hyperparameters but often with marginal returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### SGD with regression\n",
    "\n",
    "Below I set up X, y data predicting value (housing price) from the remaining variables. There are ~75,000 rows, with 170 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Import the modeling classes\n",
    "\n",
    "`SGDRegressor` and `SGDClassifer` are the models used in this solution These are the very general, flexible stochastic gradient descent classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import (LinearRegression, LogisticRegression, \n",
    "                                  Lasso, Ridge,\n",
    "                                  SGDRegressor, SGDClassifier)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "import scipy.stats as stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Standardize the data\n",
    "\n",
    "Always a necessary step when performing regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Run the gridsearch on parameters\n",
    "\n",
    "SGDRegressor and SGDClassifier use GridSearchCV and RandomizedSearchCV (which will be introduced in the next section) to find the optimal parameters. If you are still unsure of how to use the GridSearchCV, please look this up in the sklearn documentation. I want you to get used to looking things up online - you will be doing this literally every day on the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Visualize or otherwise look at the SGDRegression results\n",
    "\n",
    "How you choose to examine the results is up to you. Visualizations are always a good idea, but not entirely neccessary (or easy) in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "### RandomizedSearchCV\n",
    "\n",
    "This class is very similar to GridSearchCV in the way it is initialized with parameters. The big difference is that instead of searching across a strictly specified grid, it searches over random values that are defined by distributions.\n",
    "\n",
    "Below I have set up for you an example of parameters and calls to the class.\n",
    "\n",
    "    uniform: this random variable is from scipy.stats and it pulls random values from the uniform distribution from 0.01 to 20000\n",
    "    sgd_rand_params: the only difference here is that alpha gets the random variable. It will pull random values from that distribution\n",
    "    RandomizedSearchCV: this takes an n_iter parameter that specifies how long to search over random values\n",
    "    \n",
    "RandomizedSearchCV is often faster than GridSearchCV while getting the same optimal parameteres. However, this is not _always_ the case. As far as I know it is not completely random, but rather begins to favor values that are closer to its current optimum.\n",
    "\n",
    "For more information, see:\n",
    "\n",
    "[RandomizedSearchCV documentation](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.RandomizedSearchCV.html#sklearn.grid_search.RandomizedSearchCV)\n",
    "\n",
    "[scipy.stats available distributions and functions](http://docs.scipy.org/doc/scipy/reference/stats.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uniform = stats.uniform(loc=0.01, scale=20000)\n",
    "\n",
    "sgd_rand_params = {\n",
    "    'loss':['squared_loss'],\n",
    "    'penalty':['l1','l2'],\n",
    "    'alpha':uniform\n",
    "}\n",
    "\n",
    "sgd_reg = SGDRegressor()\n",
    "sgd_reg_rand_gs = RandomizedSearchCV(sgd_reg, sgd_rand_params, cv=5, verbose=2, n_iter=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Visualize/examine the results from your RandomizedSearchCV results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### SGDClassifier\n",
    "\n",
    "Using either GridSearchCV or RandomizedSearchCV, set up an X, y classification problem to look at. Depending on the data you loaded in, you may need to get new data. \n",
    "\n",
    "Find the optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Visualize/examine the results of the classifier\n",
    "\n",
    "Just like above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [dsi]",
   "language": "python",
   "name": "Python [dsi]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
