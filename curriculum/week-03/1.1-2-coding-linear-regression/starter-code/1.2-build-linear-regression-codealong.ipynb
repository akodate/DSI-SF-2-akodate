{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding linear regression by hand\n",
    "\n",
    "---\n",
    "\n",
    "Start by creating the \"simple linear regression\" which means the prediction of an target variable by one predictor variable.\n",
    "\n",
    "If $Y$ is our target variable and $X$ is our predictor variable, then the function defining the relationship between $X$ and $Y$ is:\n",
    "\n",
    "### $$y = \\beta_0 + \\beta_1*x_1 + \\epsilon_1$$\n",
    "\n",
    "Where $\\beta_0$, the first coefficient, is an offset commonly known as the intercept. Without the intercept term the regression line would always have to pass through the origin, which is almost never an optimal fit.\n",
    "\n",
    "$\\beta_1$ is the coefficient that multiplies the values of $X_1$ to estimate as close as possible the values of $Y$. Each value of $X_1$ is multiplied by the same coefficient as linear regression models a _linear_ relationship between our predictor and target variable. I've denoted $X$ as $X_1$ here to indicate it is the \"first\" variable we are modeling $Y$ with. In multiple regression there would be more than one $X$.\n",
    "\n",
    "$\\epsilon_1$ is the noise, or error. This is the difference between the predicted and true values that are unexplained by $X$ in the regression.\n",
    "\n",
    "---\n",
    "\n",
    "### Load real estate data\n",
    "\n",
    "For this lab you'll be using a very simple dataset on housing prices. Load it from wherever you've put in on your computer. My path is below.\n",
    "\n",
    "The variables are:\n",
    "\n",
    "    sqft: the size of the house in sq. ft\n",
    "    bdrms: number of bedrooms\n",
    "    age: age in years of house\n",
    "    price: the price of the house\n",
    "    \n",
    "I recommend converting price into units of 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "house = '/Users/alex/Desktop/DSI-SF-2-akodate/datasets/housing_data/housing-data.csv'\n",
    "house = pd.read_csv(house)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Target and predictor variables\n",
    "\n",
    "Define a target variable and predictor variable from the dataset. Often target and predictor are called dependent and independent variables in the context of linear regression. There are many different names for variables in models, but I generally stick to target and predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sqft</th>\n",
       "      <th>bdrms</th>\n",
       "      <th>age</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2104</td>\n",
       "      <td>3</td>\n",
       "      <td>70</td>\n",
       "      <td>399900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1600</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>329900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2400</td>\n",
       "      <td>3</td>\n",
       "      <td>44</td>\n",
       "      <td>369000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1416</td>\n",
       "      <td>2</td>\n",
       "      <td>49</td>\n",
       "      <td>232000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3000</td>\n",
       "      <td>4</td>\n",
       "      <td>75</td>\n",
       "      <td>539900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sqft  bdrms  age   price\n",
       "0  2104      3   70  399900\n",
       "1  1600      3   28  329900\n",
       "2  2400      3   44  369000\n",
       "3  1416      2   49  232000\n",
       "4  3000      4   75  539900"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Simple linear regression equation\n",
    "\n",
    "Our equation for the predicted values of our target variable is:\n",
    "\n",
    "### $$\\hat{y} = \\beta_0 + \\beta_1*x$$\n",
    "\n",
    "$\\hat{y}$ is the commonly used notation for the _predicted_ value of $y$.\n",
    "\n",
    "(The $\\epsilon$ error term is gone - it is an unknowable variable (if we knew what it was, we could perfectly model our target variable).\n",
    "\n",
    "Write a function that will calculate a predicted $y$ ($\\hat{y}$) from your $x$ variable and $\\beta$ coefficients. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Plot a regression line\n",
    "\n",
    "Set $\\beta_0$ to be 0 and $\\beta_1$ to be 1. This will create a regression line that passes through the origin (0,0) at a 45 degree angle.\n",
    "\n",
    "#### Plot your target variable points and your currently poorly predicted points and regression line. \n",
    "\n",
    "Use your function above to calculate the predicted points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(You may not even be able to see the slope of the line if the magnitude of one of your variables is significantly greater than the other!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Residuals\n",
    "\n",
    "The definition of \"residuals\" in linear regression is:\n",
    "\n",
    "### $$ residual = y - \\hat{y}$$\n",
    "\n",
    "Where $y$ is the true value of our target at this observation, and $\\hat{y}$ is the predicted value of our target. Simple enough. \n",
    "\n",
    "#### Write a function to calculate residuals below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Sum of squared errors and \"least squares\" estimation\n",
    "\n",
    "Residuals are also referred to as errors: the amount \"off\" you were in your predictions.\n",
    "\n",
    "Simple linear regression is an \"ordinary least squares\" method for solving linear relationships between variables. Here the [\"least squares\"](https://en.wikipedia.org/wiki/Least_squares) means that it _minimizes the sum of the squared residuals._\n",
    "\n",
    "Why the squared residuals instead of just the absolute value of the residuals? Well, both are used â€“ absolute value of residuals is often used when there are large outliers or other abnormalities in variables. \n",
    "\n",
    "However, [under assumptions that are typically met](https://en.wikibooks.org/wiki/Econometric_Theory/Assumptions_of_Classical_Linear_Regression_Model) minimizing the sum of squared errors is equivalent to estimating the [maximum likelihood](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation).\n",
    "\n",
    "#### Write a function to calculate the sum of squared errors/residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the sum of squared errors from your initial regression line above where $\\beta_0$ equalled 0 and $\\beta_1$ equalled 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot again with a new $\\beta_0$ and $\\beta_1$ you think might be better, and calculate the SSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Minimizing the sum of squared errors\n",
    "\n",
    "Deriving the equation that minimizes the sum of squared errors in simple linear regression can be done using calculus. [See here](http://web.cocc.edu/srule/MTH244/other/LRJ.PDF) or [here](https://en.wikipedia.org/wiki/Simple_linear_regression) for descriptions of the derivation.\n",
    "\n",
    "Skipping the partial derivitaves, the formulas for the $\\beta_0$ and $\\beta_1$ that minimize the sum of squares are:\n",
    "\n",
    "### $$ \\beta_1 = \\frac{\\sum_{i=1}^n (y_i - \\bar{y} ) (x_i - \\bar{x} )}{\\sum_{i=1}^n (x_i - \\bar{x})^2} $$\n",
    "\n",
    "and\n",
    "\n",
    "### $$ \\beta_0 = \\bar{y} - \\beta_1\\bar{x} $$\n",
    "\n",
    "where $\\bar{x}$ and $\\bar{y}$ are the mean of $x$ and $y$, respectively.\n",
    "\n",
    "#### Write functions below to calculate $\\beta_0$ and $\\beta_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\beta_1$ is in fact also equivalent to:\n",
    "\n",
    "### $$ \\beta_1 = \\frac{cov(x, y)}{var(x)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the optimal $\\beta_1$ and $\\beta_0$. Confirm the two equations for $\\beta_1$ are (essentially) the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the regression with the optimal parameters and calculate the SSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Pearson's r and how it relates to simple linear regression\n",
    "\n",
    "Reall that the Pearson correlation coefficient, or Pearson's r, is defined as:\n",
    "\n",
    "### $$ cor(x, y) =\\frac{cov(x, y)}{std(x)std(y)} = \\frac{\\sum_{i=1}^n (y_i - \\bar{y} ) (x_i - \\bar{x} )}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2}\\sqrt{\\sum_{i=1}^n (y_i - \\bar{y})^2}} $$\n",
    "\n",
    "If that looks similar to the equation for $\\beta_1$ above, well, it's because it is.\n",
    "\n",
    "#### Calculate the Pearson's r between your target and predictor below\n",
    "\n",
    "For now, write your own function according to the equation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create normalized versions of your target and predictor variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate new $\\beta_0$ and $\\beta_1$ for the normalized variables and plot the regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the $\\beta_0$ coefficient and Pearson's r are _nearly_ the same, but not quite..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Pearson's r and \"degrees of freedom\"\n",
    "\n",
    "The concept of \"degrees of freedom\" comes up a lot in statistics, and can be a source of a lot of confusion. \n",
    "\n",
    "A simplified definition of degrees of freedom is: \n",
    "\n",
    "> The number of final calculations in a statistical estimate that are allowed to vary. This restriction is necessary when the statistical estimate requires one or more other statistical estimates to be measured from the sample first.\n",
    "\n",
    "Degrees of freedom lets you adjust for the size of your sample: estimates from smaller samples are adjusted more. Why do we do this, and how?\n",
    "\n",
    "Take the formula for Pearson's r for the overall population $X$ and $Y$:\n",
    "\n",
    "### $$ cor(X, Y) =\\frac{cov(X, Y)}{std(X)std(Y)}$$\n",
    "\n",
    "This is equivalent to the formula above, but imagine we had _all_ the data. When we are talking about the overall (infinite) population, there are now guaranteed properties of statistical estimates that aren't guaranteed for the sample estimate.\n",
    "\n",
    "For example, in the full population the sum of deviations from the mean are guaranteed to sum to 0. But with a small sample from this population, this is obviously not guaranteed.\n",
    "\n",
    "If we want to estimate what Pearson's r would be for the _population_ but from our _sample_, then we need to take into account these guaranteed properties of the population statistic.\n",
    "\n",
    "Because we using the mean of $x$ and the mean of $y$ to measure variance and standard deviation, to ensure that our deviations from the mean actually sum to zero we have to \"leave one measurement out\" in the calculation of the deviations. The last measurement of the deviation from the means has to make the deviations sum to zero!\n",
    "\n",
    "This means that Pearson's r has **X,Y pairs - 2 degrees of freedom**, since we lose one degree of freedom for each of the two variables to adjust their mean deviations to sum to zero.\n",
    "\n",
    "#### Calculate the Pearson's r between target and dependent variables, using keyword argument `ddof=1` in both `np.var` and `np.std`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simple linear regression where there is just one predictor, the correlation coefficient is equivalent to the $\\beta_1$ slope on $x_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### $R^2$: the \"coefficient of determination\"\n",
    "\n",
    "Though we will talk about evaluation metrics for regression in more detail in a later lecture, it is worth looking at the concept of $R^2$ while we are exploring the pieces of regression in granular detail.\n",
    "\n",
    "$R^2$ is the **amount of variance explained above baseline in your target $y$ by predictor $x$**.\n",
    "\n",
    "It is comprised of two parts: the total sum of squares and the residual sum of squares.\n",
    "\n",
    "The total sum of squares is defined:\n",
    "\n",
    "### $$ SS_{tot} = \\sum_{i=1}^n \\left(y_i - \\bar{y}\\right)^2 $$\n",
    "\n",
    "The residual sum of squares is defined:\n",
    "\n",
    "### $$ SS_{res} = \\sum_{i=1}^n \\left(y_i - f_i\\right)^2 $$\n",
    "\n",
    "Where $f_i$ is the prediction by your regression for $y$ at observation $i$.\n",
    "\n",
    "$R^2$ is then calculated:\n",
    "\n",
    "### $$ R^2 = 1 - \\frac{SS_{res}}{SS_{tot}} $$\n",
    "\n",
    "The total sum of squares is the **baseline model**: the amount of variance in $y$ we would explain if we were to predict each point of $y$ with just the mean of $y$.\n",
    "\n",
    "This is equivalent to estimating $y$ with nothing but the intercept term $\\beta_0$, which becomes the mean of $y$ (the best possible estimator of $y$ with a single value: maximum likelihood):\n",
    "\n",
    "### $$ \\hat{y} = \\beta_0 = \\bar{y} $$\n",
    "\n",
    "\n",
    "#### Plot your regression again, with a new regression line representing the baseline model\n",
    "\n",
    "Print out the SSE for the baseline and model with predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write a function to calculate $R^2$ for baseline and regression models and print them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print out the Pearson's r squared:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of simple linear regression, where there is only one predictor, the correlation coefficient squared is equal to $R^2$.\n",
    "\n",
    "Another formula, which is in my opinion clearer, explicitly calculates $R^2$ from the variance explained:\n",
    "\n",
    "### $$ R^2 = 1 - \\frac{var(residuals)}{var(y)} $$\n",
    "\n",
    "#### Calculate the $R^2$ with the \"variance explained\" formula below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Multiple linear regression\n",
    "\n",
    "It is of course rare in regression that you will only want to predict $y$ from a single predictor $x$. Multiple linear regression predicts $y$ from more than 1 $x$ variable.\n",
    "\n",
    "The formula for computing the $\\beta$ values in multiple regression is best done with linear algebra. I'm not going to go into the details of the _reason_ that this works, but if you want to see the explanation [these slides are a great resource](http://statweb.stanford.edu/~nzhang/191_web/lecture4_handout.pdf).\n",
    "\n",
    "The linear algebra formula is as follows, where $X$ is a _matrix_ of predictors $x_1$ through $x_i$ (with each column a predictor), and $y$ are the true values of $y$. There is still only 1 predicted variable:\n",
    "\n",
    "### $$ \\hat{y} = \\beta X $$\n",
    "\n",
    "_The intercept term is part of $X$! It is a column of all ones added to the columns of predictors._\n",
    "\n",
    "Written out more simply, without the linear algebra, $\\hat{y}$ is calculated:\n",
    "\n",
    "### $$ \\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n $$\n",
    "\n",
    "The calculation of the $\\beta$ values is done with the linear algebra matrix multiplication formula:\n",
    "\n",
    "### $$ \\beta = (X'X)^{-1}X'Y $$\n",
    "\n",
    "Where $X'$ is the _transposed matrix of original matrix $X$_ and $(X'X)^-1$ is the _inverted matrix_ of $X'X$.\n",
    "\n",
    "Don't worry about the linear algebra here if you don't have experience with it. It's not so important to understand. Just remember that $y$ is estimated with an intercept term and each $x$ predictor multiplied by it's own $\\beta$ value.\n",
    "\n",
    "#### Create a \"design matrix\" with the first column a column of all 1s (intercept column) and the other columns the variables in the dataset that are not the target variable\n",
    "\n",
    "This is easiest to do with pandas: add a column for the intercept first, then extract the matrix with `.values`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the beta values using the formula above\n",
    "\n",
    "**The transpose of a matrix is calculated by appending `.T` to the matrix:**\n",
    "\n",
    "    X.T\n",
    "\n",
    "**Matrices multipled in the formula should be done as the \"dot product\" with:**\n",
    "\n",
    "    np.dot(mat1, mat2)\n",
    "\n",
    "**Inverting a matrix is done with the function:**\n",
    "\n",
    "    np.linalg.inv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print out the vector of betas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate predicted $\\hat{y}$ with your predictors and $\\beta$ coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the predicted values $\\hat{y}$ against the true values $y$\n",
    "\n",
    "Also plot a line through the origin at a 45 degree angle (positive slope)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the $R^2$ of the multiple regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [dsi]",
   "language": "python",
   "name": "Python [dsi]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
